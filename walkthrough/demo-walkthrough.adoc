= Serverless Demo 
:experimental:
:imagesdir: images
:toc:
:toclevels: 4

[IMPORTANT]
.On necessary operators
====
See link:demo-setup.adoc[] for information on operators and other prerequisites that must be installed for the demo to run properly.
====

This demo centers around the conversion of a traditional (payment) service into one that is implemented serverless-ly.  It supports the "Coolstore" website the relevant architecture of which can be seen here:

image:coolstore-arch.png[]

== Setup Tips Prior to Walkthrough ==

* [OPTIONAL] Open CodeReadyWorkspaces with the devfile to ensure it is initialized at least once
** _This demo has image pre-pull support so the startup time should not be so bad_
* VSCode in the Desktop to the right of the main demo desktop


=== Desktops

==== Desktop 0

This should be offscreen

. *Hidden Shell*: This is the shell for running support commands that would otherwise confuse the demo
.. Go to the root of the `serverless-demo` directory on your local machine
.. Run the same docker command from before
+
----
docker run --rm -it -v ~/.kube:/home/jboss/.kube -v /var/run/docker.sock.raw:/var/run/docker.sock -v ~/.oh-my-zsh:/home/jboss/.oh-my-zsh -v $(pwd):/workspaces/serverless-demo -w /workspaces/serverless-demo quay.io/mhildenb/kn-demo-shell /bin/zsh
----
+
.. Ensure you are logged in as a `cluster-admin`

==== Desktop 1 (Main)

. *Shell 1*: This will be the main shell for commands to run
.. Go to the root of the `serverless-demo` directory on your local machine
.. Run this docker command from that directory (so that all the remaining commands will be run from inside a container will all the necessary tools installed)
+
----
docker run --rm -it -v ~/.kube:/home/jboss/.kube -v /var/run/docker.sock.raw:/var/run/docker.sock -v ~/.oh-my-zsh:/home/jboss/.oh-my-zsh -v $(pwd):/workspaces/serverless-demo -w /workspaces/serverless-demo quay.io/mhildenb/kn-demo-shell /bin/zsh
----
+
.. Ensure you are logged in as a `cluster-admin`
. *Window 1*: Chrome with the following tabs
.. Topology view of the Dev Project
.. Cluster's Gitea home (logged in)
. *Window 2*: Have a second window with the following tabs
.. Pipelines view of the kn-demo-cicd project to look for pipeline runs
+
. *Kafka Shell*: This will be used to show what's coming through the payment queue
.. Go to the root of the `serverless-demo` directory on your local machine
.. Run this docker command from that directory (so that all the remaining commands will be run from inside a container will all the necessary tools installed)
+
----
docker run --rm -it -v ~/.kube:/home/jboss/.kube -v /var/run/docker.sock.raw:/var/run/docker.sock -v ~/.oh-my-zsh:/home/jboss/.oh-my-zsh -v $(pwd):/workspaces/serverless-demo -w /workspaces/serverless-demo quay.io/mhildenb/kn-demo-shell /bin/zsh
----
+
.. Ensure you are logged in as a `cluster-admin`
.. Run this command to watch the queue
+
----
 oc exec -c kafka my-cluster-kafka-0 -n $dev_prj -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----

==== Desktop 2 (VSCode)

. Have VSCode opened (remote) at the root of the `serverless-demo` directory.

== Intro to Coolstore

. Open the Topology View
.. Notice all the different support services
.. Be sure to point out the *Kafka Cluster*
. Set to `Focus`
.. Now just on the services running
.. Point out that the payment service is running just like the rest
. Set up watches on the different kafka topics
.. In the `Kakfa Order Shell` run the following command:
+
----
oc exec -c kafka my-cluster-kafka-0 -n $dev_prj -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic orders
----
+
.. In the `Kafka Payment Shell` run the following command:
+
----
oc exec -c kafka my-cluster-kafka-0 -n $dev_prj -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----
+
. Click the route badge to open the website
. Buy something 
. You should see the order and the payment come through the shells
. In the website, click on Orders.  You should see the following:
+
image:initial-order-purchase.png[]


== Installing OpenShift Serverless

. In a new tab, go to Operator Hub
. Search for `Serverless`
+
image:operator-hub-serverless.png[]
+
.. Show the information about the operator
. Install the operator with all the defaults
+
image:operator-defaults.png[]
+
. Wait until operator is installed and then click on the "View Operator" button or link
+
image:operator-wait.png[]
+
. Switch to `knative-serving` project (which was automatically created by the operator)
+
image:operator-switch.png[]
+
. Create the `KnativeServing` instance for the cluster by clicking on the highlighted link
+
image:start-knative-serving.png[]
+
. Once on the page listing all the `Knative Servings` for the knative-serving project, click the "Create Knative Serving" link
.. Show different options in `Form View` and explain how this govens the defaults for how serverless deployments will behave
. Switch to `YAML View` and paste contents from link:../install/serverless/cr.yaml[the cr.yaml file in the project]
*** You can use kbd:[CMD + P] to quickly open `cr.yaml` in VS Code and copy all the contents
+
image:knative-cr-interesting.png[]
+
. Click `Create` button at bottom of page
. Point out that the `Serverless` drawer now appears in the UI
.. There's not much to see yet, but you might click into  it
.. Talk about OpenShift is tightly integrated with OpenShift Serving and new options are available all over the cluster, as will be seen in the next section
. Before moving onto the next section, make sure knative-serving is ready by typing in the `shell`
+
----
oc wait --for=condition=InstallSucceeded knativeserving/knative-serving --timeout=6m -n knative-serving
----
+
.. When knative-serving is fully installed, the command will return with:
+
----
knativeserving.operator.knative.dev/knative-serving condition met
----

== Creating Knative Service

. Ensure that the `Topology View` of the `kn-demo-dev` project is visible in the `Browser Window`
+
image:topology-view.png[]
+
. Set the payment-traditional service down to 0
.. If you want to do this from the `shell` instead:
+
----
oc scale --replicas=0 deployment/payment-traditional -n $dev_prj
----
+
. To prove that nothing is addressing orders as they come in, run the following command in the `shell` to simulate the placement of an order:
+
----
cat $DEMO_HOME/example/order-payload.json | oc exec -i -c kafka my-cluster-kafka-0 -n $dev_prj -- /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic orders
----
+
.. You should see an order in the `Kafka Order Shell` but no corresponding payment processing in the `Kafka Payments Shell`
.. You can also navigate to the orders page of the Coolstore website and see the order not processed
+
image:unprocessed-order.png[]
+
. Next right-click on the `focus` grouping to add a new application from git
+
image:add-kn-service-git.png[]
+
. Get the repo URL from the following command
+
----
echo "https://$(oc get route gitea -n $cicd_prj -o jsonpath='{.spec.host}')/gitea/coolstore"
----
+
. Fill in the form with the following:
** _Git Repo URL_: $REPO
** _Git Reference_ (advanced options): serverless-demo
** _Builder_: Java, `openjdk-11-ubi8`
** _Name_: payment
** _Resources_: Knative Service
** _Pipelines_: Do not add Pipeline
** _Build Configuration_ (advanced options): Uncheck all
** _Scaling_ (advanced options): 
*** _Concurrency Target_: 1
*** _Concurrency Limit_: 1
+
image:import-from-git-1.png[]
image:import-from-git-2.png[]
image:import-from-git-3.png[]
image:import-from-git-4.png[]
+
. In the `hidden shell` run the following command to create and annotate the revision so that we get a badge for CRW
+
----
kn service update payment -n $dev_prj --image quay.io/rhdevelopers/knative-tutorial-greeter:quarkus --annotation "app.openshift.io/vcs-ref=serverless-demo" --annotation "app.openshift.io/vcs-uri=https://github.com/hatmarch/coolstore.git" --revision-name "{{.Service}}-initial"
----
. When the revision comes up you will see the badge for editing the code in CRW
+
image:initial-revision-badge.png[]

== Turn payment service "Serverless" in CodeReady Workspaces

[IMPORTANT]
.Seeding the CRW
====
The workspace needs to download a lot of maven packages to get the language server to run.  You can cut down on this time if you run the "Seed Cache" command earlier in the process than the instructions indicate.

If you find yourself waiting around for the the "Activating Quarkus" prompt to go away, then this might speed things up

image:activating-quarkus.png[]

====

. Click on the CRW badge that is on the new payment knative service Revision
.. This will launch CRW.  It will take a little while to load the workspace from the Devfile
. While waiting, you could show the `Devfile` in VSCode (use kbd:[CMD + p] to quickly open `devfile`)
. Once in CRW, quickly open the `PaymentResource` by using kbd:[CMD+p]
.. Show where the PaymentResouce is in the project
.. Mention that it's written in Quarkus
.. Explain
*** Emitter (and kbd:[CMD + p] `application.properties`) to show integration with Kafka
*** `onMessage`: This is called from watching for incoming orders
*** `handleCloudEvent`: This is called when orders are found
*** `pass`, `fail`, `createPayment`: These functions create a payload for the emitter
. Edit the PaymentResource such that it looks like the diff on the right
** *NOTE:* You can also copy and paste from link:../example/payment-knative/PaymentResource.java[example/payment-knative/PaymentResource.java]
+
image:knative-diffs.png[]
+
. Open the side bar to and select `Seed Cache` to ensure a quicker local compile
. Then select `New Terminal` to open a terminal
+
image:crw-sidebar-terminal.png[]
+
. In the terminal, run this command to run in live mode
+
----
cd coolstore/payment-service
mvn quarkus:dev
----
+
. When the code starts running, CRW will prompt you that different routes and ports are open
.. When the main route comes up, copy this route
** [red]#NOTE:# You may have to refresh the page a couple times before it comes up
+
image:preview-route.png[]
+
. In the `shell`, set the CRW_URL variable with the route that was published by CRW
+
----
CRW_URL=<COPIED ROUTE>
----
+
. Access the route by posting to it with this command
+
----
curl -i -H 'Content-Type: application/json' -X POST -d @$DEMO_HOME/example/order-payload.json $CRW_URL
----
+
. You should now see something posted to the `Kafka Payments Shell` with a tell-tale change of:
+
----
... "status":"COMPLETED (Serverless Service)"
----

== Commit and Trigger Pipelines Build

. From within CRW, go to the `Git` window, enter a commit message and stage the `PaymentResource` with the `+` button
+
image:checkin-message.png[]
+
. In the CRW terminal (potentially after interrupting `mvn quarkus:dev` with kbd:[ctrl + c]) run the following commands:
+
----
git commit -m"Setup payment service for serverless and EDA"
git push origin
----
+
. You will be prompted for a username and password
** *username*: `gitea`
** *password*: `gitea`
. _Immediately_ switch to the Pipelines Browser window and show that a `PipelineRun` has been triggered
+
image:pipeline-run.png[]
+
. Click on the `PipelineRun` link (highlighted above) and show the PipelineRun details
+
image:pipeline-run-details.png[]
+
. Explain the different build stages
. Click on the logs to follow
.. BETTER: in the `shell` run the following command to follow the build (and maximize shell):
+
----
tkn pr logs -L -f -n $cicd_prj
----
+
. When the pipeline finishes, it will output the URL of the knative service which we'll use in the next section
+
image:tkn-kn-url.png[]
+
. Copy the highlighted URL and set the following variable in the `shell`:
+
----
KN_URL=<URL copied from above>
----

== OPTIONAL: Single Knative Function Invoke

. Open the `Topology View` of the browser tab and ensure the knative service can be seen even when the `shell` is open
. Point out the the knative service is set to 0 instances
+
image:pre-invoke-shell.png[]
+
. Put a couple newlines in the `Kafka Payments Shell`
. In the `shell` run the following command to invoke the service:
+
----
curl -i -H 'Content-Type: application/json' -X POST -d @$DEMO_HOME/example/order-payload.json $KN_URL
----
+
. You should see the pod start up
. Once running, you should see a new entry in the `Kafka Payments Shell`
. Continue to watch the service until the pod stops and the number of instances goes back to 0

== Install Knative Eventing

. Duplicate the `Topology View` tab and switch to the `Administrator Perspective`
. Click on Operators > Installed Operators on the left and switch to the `knative-eventing` project
. Click the Knative Eventing link
+
image:knative-eventing-start.png[]
+
. Click 'Create Knative Eventing`
.. Show different options in `Form View` and explain what you can
. We can just use all the defaults, so just click `Create`
. Before moving on, make sure eventing has finished by typing the following in the shell:
+
----
oc wait --for=condition=InstallSucceeded knativeeventing/knative-eventing -n knative-eventing --timeout=6m
----
+
. It's safe to continue when the shell returns and prints:
+
----
knativeeventing.operator.knative.dev/knative-eventing condition met
----
+
. Next create a `Knative Kafka` integration by clicking on the `Knative Kafka` tab
+
image:knative-kafka-tab.png[]
+
. Click `Create KnativeKafka`
. From the `Form View`, explain that this is what allows kafka messages to be translated into knative events for the EDA we've talked about
. The eventing can be configured from the Form View.  Fill in the fields like this and click create:
** *Channel > Bootstrap Servers*: `my-cluster-kafka-bootstrap.kn-demo-dev:9092`
** *Channel > Enabled*: `true`
** *Source > Enabled*: `true`
+
image:knative-kafka-form.png[]
+
. The KnativeKafka should be created almost instantly.  You know it's safe to use once this condition can be seen
+
image:knative-kafka-install-success.png[]

== Bind Kafka Events to Knative Service

. Go back to the `Topology View` tab of the `kn-demo-dev` project
. Drag out the connector arrow from the payment knative service to add an `Event Source` as shown below:
+
image:create-event-source.png[]
+
. Fill out the Event Source Details as follows:
+
image:kafka-event-sources-1.png[]
image:kafka-event-sources-2.png[]
image:kafka-event-sources-3.png[]
image:kafka-event-sources-4.png[]
+
. Click `Create`.  You will be returned to the `Topology View`
. Open the Coolstore website is a separate (small) window.
** Make sure topology view can be seen
** Make sure Kafka Queues can be seen
+
image:suggested-layout-coolstore-event.png[]
+
. Order something from the website and click `Checkout`
. The payment knative service should spin up and the order and payment topics should have messages registered
. Switch to the Order tab in coolstore to show that this order has been processed

== Scaling and Concurrency

. From the Topology view, go to the `payment` service, open the details, and select `Edit payment` menu item
+
image:edit-payment-ksvc.png[]
+
. To remind the audience of the scaling limits, scroll down to the bottom of the ksvc details until you get to the `Advanced` section.  There click on the `Scaling` link
+
image:ksvc-scaling.png[]
+
. Show the scaling details and highlight concurrency limits
+
image:import-from-git-4.png[]
+
. Hit `Cancel` to go back to the Topology View with the payment service in focus
. Under `Display Options` select `Pod Count` to that count can be stressed
+
image:pod-count-options.png[]
+
. _Whilst keeping pod-count payment service in focus (with details), and the `Kafka Payment Shell` visible_: from the `shell` run the following command to send 50 concurrent requests to the payment service
** NOTE: `KN_URL` should already have been set from previous sections but if you don't have it you can get it with `KN_URL=$(oc get rt payment -o jsonpath='{.status.url}')`
+
----
hey -n 50 -c 50 -t 60 -m POST -D $DEMO_HOME/example/order-payload.json -T "application/json" $KN_URL
----
+
. The count on the payment ksvc should increase to near 50
. Show the `hey` histogram to get a sense of timings
+
image:scaling-and-histogram.png[]

== Revisions and Traffic Splitting

. Next, in the payment details highlight the revision list for `payment`
+
image:revision-list.png[]
+
. _Whilst keeping the revision list visible_: Add a native revision with `kn command` from the `shell`
+
----
kn service update payment -n $dev_prj --image quay.io/mhildenb/homemade-serverless-native:initial-service-1.1 --revision-name "{{.Service}}-native"
----
+
. Notice that payment-native is now set to get 100% of the traffic
** NOTE: You may need to recent payment in the `Topology View` window
+
image:new-native-revision.png[]
+
. Show that traffic is going 100% to the new native service by running the same `hey` command in the `shell`
+
----
hey -n 50 -c 50 -t 60 -m POST -D $DEMO_HOME/example/order-payload.json -T "application/json" $KN_URL
----
+
. Be sure to show the new "native" messages coming through in the `Kafka Payments Shell`
. Also show the histogram.  Should be responding a little bit faster
. Finally, let's split traffic between the two revisions, click on `Set Traffic Distribution`
. Adjust the Distribution between the previous (non-native) revision as shown:
** NOTE: Click 'Add Revision` link to add a new line
+
image:edit-revisions.png[]
+
. Click Save and go back to the `Topology View` refocusing as necessary
+
image:traffic-split.png[]
+
. _Whilst keeping pod-count payment service in focus (with details), and the `Kafka Payment Shell` visible_: from the `shell` run the same `hey` command to send 50 concurrent requests to the payment service
+
----
hey -n 50 -c 50 -t 60 -m POST -D $DEMO_HOME/example/order-payload.json -T "application/json" $KN_URL
----
+
. Show the hey histogram and `Kafka Payment Shell` messages as evidence of the split
+
image:split-histogram.png[left,300]
image:split-messages.png[right,400]

== Knative Functions

[IMPORTANT]
.Latest Kafka Certificates Needed To Run Locally
====
To make sure you have the latest kafka certs for running the Function locally, make sure you run this in all terminals that you will be running the service FIRST

----
oc get pods -n openshift-operators -o name | grep amq-streams | xargs oc delete -n openshift-operators

# Wait until Kafka is reinitialized

# Reinstate your local properties
. $DEMO_HOME/scripts/shell-setup.sh
----
====

. Switch to VSCode Desktop
+
. In the terminal, navigate to the following (empty) directory
+
----
mkdir $DEMO_HOME/coolstore/payment-func
cd $DEMO_HOME/coolstore/payment-func
----
+
. Initialize a new function by running the following command in the terminal
+
----
kn func create -l quarkus -t events
----
+
. Then override the default func.yaml with values we prepared earlier (about the destination image name, etc)
+
----
cp -f $DEMO_HOME/example/payment-func/func.yaml .
----
+
. Now remove all the highlighted files
+
image:kn-func-files-to-delete.png[]
+
. Next, open the `Function.java` file and show the stuff that was added by default
** Highly @Funq context
. Change `Input` to be of type `Order` and remove Input.java
** Explain that built into the function machinery is to map the body of a cloud event to POJOs
. Use kbd:[CMD + p] to open `payment-cloud.http` quickly
** Show the structure of the body
. Create the following POJOs or copy them into the payment-func directory (and show them) by running
+
----
cp $DEMO_HOME/example/payment-func/src/main/java/functions/Order.java $DEMO_HOME/coolstore/payment-func/src/main/java/functions
cp $DEMO_HOME/example/payment-func/src/main/java/functions/CreditCard.java $DEMO_HOME/coolstore/payment-func/src/main/java/functions
----
+
. We are needing to connect to kafka, so let's add that library to our project
+
----
mvn quarkus:add-extension -Dextensions=reactive-messaging-kafka
----
+
. Next edit the function file so that it looks like this (perhaps using link:example/payment-knative/PaymentResource.java[example/payment-knative/PaymentResource.java] as a reference on the side):
** Alternatively, run this command:
+
----
cp -f $DEMO_HOME/example/payment-func/src/main/java/functions/Function.java $DEMO_HOME/coolstore/payment-func/src/main/java/functions/Function.java
----
+
image:func-diff-1.png[]
image:func-diff-2.png[]
+
. Open the `application.properties` and notice there is only one there by default
. Add the properties necessary for connecting to kafka by overwriting it with this command
+
----
cp -f $DEMO_HOME/example/payment-func/src/main/resources/application.properties $DEMO_HOME/coolstore/payment-func/src/main/resources/application.properties
----
+
. Run the service locally to prove that it's working
. Open a new split terminal to use as the `Kafka Payment Shell` and run the following command in it
+
----
oc exec -c kafka my-cluster-kafka-0 -n $dev_prj -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----
+
. OPTIONAL: To ensure you have the latest certs for accessing the Kafka Cluster, run this command in the terminal 
. In the orginal shell, run the following to start the service running locally 
+
----
mvn quarkus:dev -Dsuspend
---- 
+
. Attach the debugger and set a breakpoint in the Function 
. Use kbd:[CMD + p] to open `payment-cloud.http` quickly and click on "Send" code lens
+
image:vscode-order-send.png[]
+
. The function should execute and the output should be seen in the `Kafka Payment Shell`
. Deploy the function using the following command:
+
----
kn func deploy -v -n $dev_prj
----
+
. When the deployment completes, switch to `Desktop 1` to show the Topology View
. Select `Application: All Applications`
. Find the function revision and kbd:[shift]-drag the revision into the focus group
. Select `Application: Focus`
. Next re-wire the kafka source
. Create the kafka event source using this command:
** [red]#NOTE: The UI for kafka event sources appears to be broken so you can't YET move the connector to the Service#
+
image:move-connector.png[left, 300]
image:move-error.png[right, 300]
+
. Instead, we'll delete it and create a new one manually in the `shell`
** NOTE: It's important you delete the old kafka source or you'll get multiple updates in the channel
----
oc delete -n $dev_prj -f $DEMO_HOME/install/knative-eventing/orders-event-source-func.yaml
oc apply -n $dev_prj -f $DEMO_HOME/install/knative-eventing/orders-event-source-func.yaml
----
+
. _Whilst making sure the `Kafka Payment Shell` and Topology View with Knative Func is visible_: Make a purchase from the coolstore.
. Notice the new message
+
image:function-final.png[]

= POTENTIAL FUTURE EXPANSION

== Service Autoscaling

[NOTE]
====
The knative-serving attribute scale-to-zero-grace-period is a “dynamic parameter” i.e. any updates to this value are reflected immediately to all its consumers; while all other parameters are static parameters i.e. change to it need a restart of the autoscaler deployment of knative-serving namespace.
==== 

TODO: See link:https://knative.dev/docs/serving/samples/autoscale-go/index.html[here]
Then reopen the website

== Service Pinning

TODO: See link:https://redhat-developer-demos.github.io/knative-tutorial/knative-tutorial-basics/0.7.x/02-basic-fundas.html#_service_pinned_to_first_revision[here]

= TROUBLESHOOTING =

== 500 errors

[NOTE]
====
This may no longer be an issue, as the payment service is now not creating its own Kafka Producer but instead using the one that is part of the smallrye reactive kafka library.  The event driven approach (as opposed to the old imperative one) should be addressed
====

You may notice 500 errors, particularly if you send multiple requests under load:

image:500-errors.png[]

I believe this is because there is currently a race condition when the second request hits a pod where the payment topic (`producer` in the code) is not fully setup in the payment service (thus a null pointer).  Looks like the first exception happens in the `pass` function but this is caught in the handleCloudEvent function, only for the `fail` event to use the `producer` null pointer to try to log a failure at which time a new uncaught exception is raised.

If you set the concurrently limit to 1, you should be able to demonstration that this error doesn't happen with hey

== Getting logs of Knative service

The epheral nature of the knative service can make it hard to capture logs of the service, particularly if you notice that the service had issues after it's gone.

Aside from setting up Elasticsearch to retain all logs, you can consider using `stern` in the background.  Using the .devcontainer that is run from within VSCode, you can have the following command running in a background terminal:

----
stern -l serving.knative.dev/service=payment
----

To see all the logs from revision 1 of the payment service (-1 represents the revision number I believe).  This command will include logs from all containers associated with the pod (such as `queue-proxy`).  If you only want the deployed code itself to log, add the `-c user-container` flag

== Viewing and Modifying Order (MongoDB) Database

You cannot connect to the mongodb instance using the latest plain adminer container.  Instead you need to follow the special instructions below.  If you my version of adminer does not work for you, you can attempt to follow <<Updating your own Adminer image,these instructions>> for creating a new image yourself from the latest.

. Start port forwarding to the mongodb service
+
----
oc port-forward -n coolstore svc/order-database 27017:27017
----
+
. Run the modified adminer pod
** NOTE: `quay.io/mhildenb/myadminer:1.1` is a version 4.7.6 adminer container that I've updated to support this
+
----
docker run -p 8080:8080 -e ADMINER_DEFAULT_SERVER=docker.for.mac.localhost quay.io/mhildenb/myadminer:1.1
----
+
. Login as shown
+
image:adminer-mongo-password.png[]
+
. You should now have access to the mongo database with the ability to list and edit entries:
+
image:adminer-mongo-edit.png[]

== Insecure ImageRegistry 

Might be solved as per link:https://github.com/knative/serving/issues/2136[here] but can't get the controller pod to take the new environment variable

Looks like it has something to do with the labels.  If the sha is used instead it seems to work properly.  You can find the sha like this:
----
$ oc get istag/payment:latest -o jsonpath='{.image.dockerImageReference}'
image-registry.openshift-image-registry.svc:5000/user1-cloudnativeapps/payment@sha256:21ca1acc3f292b6e94fab82fe7a9cf7ff743e4a8c9459f711ffad125379cf3c7
----

And then apply it as a service like this:
----
kn service create payment --image $(oc get istag/payment:initial-build -o jsonpath='{.image.dockerImageReference}') --label "app.kubernetes.io/part-of=focus" --revision-name "{{.Service}}-{{.Generation}}" --annotation sidecar.istio.io/inject=false --force
----

----
oc port-forward <image-registry-pod> -n openshift-image-registry 5001:5000
----

To get the cert as a pem file, do this:
----
openssl s_client -showcerts -connect localhost:5001 </dev/null 2>/dev/null|openssl x509 -outform PEM >mycertfile.pem
----

= APPENDIX

== Older Walthroughs

Here are walkthroughs for earlier versions of OpenShift/OpenShift Serverless

[cols="2,1,1"]
|===
|[red]#Versions# |[red]#Year# |[red]#Link#
|*OpenShift < 4.6 (OpenShift Serverless < 1.11)*
|2020
|link:demo-walkthrough-2020.adoc[here]

|===

== Copying OpenShift images to public repositories

If you have images that you've compiled on an OpenShift cluster and you want to pull them out of the local image stream to something like `quay.io`, you can use one of the following approaches to copy the images out of openshift.  Both use the `skopeo` command which is installed by default in the .devcontainer.  

For both examples, it assumes the copying of a payment service.  As such, note the following for the different variables:

* USER: your username for the public repository
* PASSWORD: your password or TOKEN for the public repository
* PROJECT: the project your image stream lives in (such as coolstore)
* IMAGE_DEST: Replace this with your repository, project, image-name, and version, example: `quay.io/mhildenb/homemade-serverless-java:1.0`: 

=== Image Registry is exposed publicly 

You need only run the following command:

----
skopeo copy --src-creds "$(oc whoami):$(oc whoami -t)" --dest-creds "${USER}:${PASSWORD}" docker://$(oc get is payment -o jsonpath='{.status.publicDockerImageRepository}'):latest docker://{IMAGE_DEST}       
----

=== Image Registry is private

If instead you need to copy from an image registry that is not exposed outside the cluster, you must instead do the following:

. Port forward to openshift's internal image registry
+
----
oc port-forward svc/image-registry -n openshift-image-registry 5000:5000
----
+
. Then in a separate shell, run the following command
+
----
skopeo copy --src-creds "$(oc whoami):$(oc whoami -t)" --src-tls-verify=false --dest-creds "${USER}:${PASSWORD}" docker://localhost:5000/${PROJECT}/payment:latest docker://{IMAGE_DEST}
----

== Updating your own Adminer image

There are two reasons why the normal adminer image cannot connect to the mongo database:

1. It requires a newer version of php integration with MongoDB
2. The mongoDB is not setup with a user and a password (Adminer does not allow access to such databases by default for security reasons)

To update the latest adminer image to be able to connect to the userless MongoDB follow these instructions:

. Run an instance of the adminer container as follows:
+
----
docker run -it -u root --name my_adminer adminer:latest sh 
----
** NOTE: If an instance of the container is already running you can use the `docker exec -it` command instead
+
. Then from inside the container run
+
----
apk add autoconf gcc g++ make libffi-dev openssl-dev
pecl install mongodb
echo "extension=mongodb.so" > /usr/local/etc/php/conf.d/docker-php-ext-mongodb.ini
----
+
. Next add a plugin as per link:https://nerdpress.org/2019/10/23/adminer-for-sqlite-in-docker/[This site].  It will require you to create a login-password-less.php file in the `/var/www/html/plugins-enabled/` directory
+
[CONTENTS]
====
----
<?php
require_once('plugins/login-password-less.php');

/** Set allowed password
 * @param string result of password_hash
 */
return new AdminerLoginPasswordLess(
    $password_hash = password_hash("admin", PASSWORD_DEFAULT)
);
----
====
+
. now commit this container as a new image
+
----
docker commit my_adminer myadminer:1.1    
----

