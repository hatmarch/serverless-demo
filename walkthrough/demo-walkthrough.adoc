= Serverless Demo 
:experimental:
:imagesdir: images
:toc:
:toclevels: 4

[IMPORTANT]
.On necessary operators
====
See link:demo-setup.adoc[] for information on operators and other prerequisites that must be installed for the demo to run properly.
====

This demo centers around the conversion of a traditional (payment) service into one that is implemented serverless-ly.  It supports the "Coolstore" website the relevant architecture of which can be seen here:

image:coolstore-arch.png[]

== Setup Tips Prior to Walkthrough ==

* [OPTIONAL] Open CodeReadyWorkspaces with the devfile to ensure it is initialized at least once
** _This demo has image pre-pull support so the startup time should not be so bad_
* VSCode in the Desktop to the right of the main demo desktop


=== Desktops

==== Desktop 0

This should be offscreen

. *Hidden Shell*: This is the shell for running support commands that would otherwise confuse the demo
.. Go to the root of the `serverless-demo` directory on your local machine
.. Run the same docker command from before
+
----
docker run -it -v ~/.kube:/home/jboss/.kube -v /var/run/docker.sock.raw:/var/run/docker.sock -v ~/.oh-my-zsh:/home/jboss/.oh-my-zsh -v $(pwd):/workspaces/serverless-demo -w /workspaces/serverless-demo quay.io/mhildenb/kn-demo-shell /bin/zsh
----
+
.. Ensure you are logged in as a `cluster-admin`

==== Desktop 1 (Main)

. *Shell 1*: This will be the main shell for commands to run
.. Go to the root of the `serverless-demo` directory on your local machine
.. Run this docker command from that directory (so that all the remaining commands will be run from inside a container will all the necessary tools installed)
+
----
docker run -it -v ~/.kube:/home/jboss/.kube -v /var/run/docker.sock.raw:/var/run/docker.sock -v ~/.oh-my-zsh:/home/jboss/.oh-my-zsh -v $(pwd):/workspaces/serverless-demo -w /workspaces/serverless-demo quay.io/mhildenb/kn-demo-shell /bin/zsh
----
+
.. Ensure you are logged in as a `cluster-admin`
. *Window 1*: Chrome with the following tabs
.. Topology view of the Dev Project
.. Cluster's Gitea home (logged in)
. *Window 2*: Have a second window with the following tabs
.. Pipelines view of the kn-demo-cicd project to look for pipeline runs
+
. *Kafka Shell*: This will be used to show what's coming through the payment queue
.. Go to the root of the `serverless-demo` directory on your local machine
.. Run this docker command from that directory (so that all the remaining commands will be run from inside a container will all the necessary tools installed)
+
----
docker run -it -v ~/.kube:/home/jboss/.kube -v /var/run/docker.sock.raw:/var/run/docker.sock -v ~/.oh-my-zsh:/home/jboss/.oh-my-zsh -v $(pwd):/workspaces/serverless-demo -w /workspaces/serverless-demo quay.io/mhildenb/kn-demo-shell /bin/zsh
----
+
.. Ensure you are logged in as a `cluster-admin`
.. Run this command to watch the queue
+
----
 oc exec -c kafka my-cluster-kafka-0 -n $dev_prj -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----

==== Desktop 2 (VSCode)

. Have VSCode opened (remote) at the root of the `serverless-demo` directory.

== Installing Knative Serverless

== Creating Knative Service

. Set the payment-traditional service down to 0
.. If you want to do this from the commandline instead:
+
----
oc scale --replicas=0 deployment/payment-traditional -n $dev_prj
----
+
. Next add to the `focus` application from git
+
image:add-kn-service-git.png[]
+
. Get the repo URL from the following command
+
----
REPO=echo "https://$(oc get route gitea -n $cicd_prj -o jsonpath='{.spec.host}')/gitea/coolstore"
----
+
. Fill in the form with the following:
** _Git Repo URL_: $REPO
** _Git Reference_ (advanced options): serverless-demo
** _Builder_: Java, `openjdk-11-ubi8`
** _Name_: payment
** _Resources_: Knative Service
** _Pipelines_: Do not add Pipeline
** _Build Configuration_ (advanced options): Uncheck all
** _Scaling_ (advanced options): 
*** _Concurrency Target_: 1
*** _Concurrency Limit_: 1
+
image:import-from-git-1.png[]
image:import-from-git-2.png[]
image:import-from-git-3.png[]
image:import-from-git-4.png[]
+
. In the `hidden shell` run the following command to create and annotate the revision so that we get a badge for CRW
+
----
kn service update payment --image quay.io/rhdevelopers/knative-tutorial-greeter:quarkus --annotation "app.openshift.io/vcs-ref=serverless-demo" --annotation "app.openshift.io/vcs-uri=https://github.com/hatmarch/coolstore.git" --revision-name "{{.Service}}-initial"
----
. When the revision comes up, then click on the badge to start editing the code

== Turning a normal service into a Knative Service

. Edit the code in CRW removing the struck through lines
. Run the code locally and prove that it has access to the kafka topic

== Knative Functions

. Clone the git repo of the openshift cluster as a submodule of the VS Code project
+
----
git submodule add -b serverless-demo https://$(oc get route gitea -o jsonpath='{.spec.host}' -n $PROJECT_PREFIX-cicd)/gitea/coolstore func
----
+
. Create a directory into which to create the function
+
----
mkdir -p func/payment-func
cd func/payment-func
----
+
. Initialize the function
+
----
kn func create -l quarkus -t events
----
+
TODO: Fill in editing and local running steps
+
. Deploy the function using the following command
+
----
kn func deploy -v -n $PROJECT_PREFIX-dev
----
+
. By default the function is set to have a 200Mi limit of memory which appears to be too little for the quarkus function.  If you notice OOMKilled errors or just want to be safe, run this command to increase the function's memory limits (via the knative service)
+
----
# replace this with the name of your function
FUNC_SVC_NAME=payment--func
kn service update $FUNC_SVC_NAME --limit 'memory=400Mi' -n $PROJECT_PREFIX-dev
----
+
. Create the kafka event source using this command:
** [red]#NOTE: The UI for kafka event sources appears to be broken#
+
----
oc apply -f $DEMO_HOME/install/knative-eventing/orders-event-source.yaml
----
+
. Ensure that old payment service is not longer running or attached to knative event
+
. Make sure you have a shell that is showing the payment messages:
+
----
oc exec -c kafka my-cluster-kafka-0 -n kn-demo-dev -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----
+
. Purchase an item from the website as you would do normally.

=== Run Funcion Locally

[red]#These instructions need to be updated for the kafka producer side of things: it's not simple as a port-forward

. To run locally, it's important to add a profile that overrides the kafka bootstrapper property location.  In `application.properties` add the following
+
----
%dev.mp.messaging.outgoing.payments.bootstrap.servers=localhost:9092
----
+
. Then forward to the kafka bootstrapper
----
oc port-forward svc/my-cluster-kafka-bootstrap 9092:9092 -n ${PROJECT_PREFIX}-dev
----
+
. Show messages as they appear in the payment queue
+ 
----
oc exec -c kafka my-cluster-kafka-0 -n ${PROJECT_PREFIX}-dev -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----

== Payment Service on CRC 

In this section well show a payment service that is running to poll the kafka 

=== Setup Code Ready Containers 

. Download Code ready containers
. Unzip and move the binary to a location in your path
. Save the pull secret to a location in your home directory
. Configure the crc as follows:
+
----
crc config set cpus 8
crc config set memory 10486
crc config set pull-secret-file <PATH_TO_PULL_SECRET>
----
+
. Run `crc setup`
. Run `crc start` and once started record the login information presented

=== Optional: Show UI Installation of necessary operators

For this section you will need two windows: a command line shell and the web browser (for the console)

. From the console, run `crc console` which should show the OpenShift console in the running CRC instance
** NOTE:  Make sure you have the kube admin login password handy
. Go to _Operators > Operator Hub_
. Search for Knative and select the OpenShift Serverless Operator
. Select install (for all projects)
. OPTIONAL: Search for Kafka and select the AMQ Streams operator
. Create a new namespace called `knative-serving` and select this as the current project
. Navigate to the _Operators > Installed Operators_ tab and wait until OpenShift Serverless is successfully copied
. Click on the _Knative Serving_ link and then press the _Create Knative Serving_ button
+
image:knative-serving-cr-console.png[]
+
. A default CR YAML UI will appear. Point out some of the more interesting elements of the CR (as well as the help on the right side of the screen)
+
image:knative-cr-interesting.png[]
+
. Click the _CREATE_ button
. Wait for a bit and after a while, a new tab should appear in the left OpenShift drawer navigation, namely the _Serverless_ tab
. Click on the tab and show what's underneath
+
image:serverless-drawer.png[]

=== Build and install payment service on CRC

==== Build Payment

. Make sure the CRC is running and you have the appropriate login string
. Run the following commands in a shell
+
----
source scripts/shell-setup.sh
code .
----
+
. From a shell in VSCode, run the following to install the necessary prerequisites (NOTE the flags to the `install-prereq.sh` command.  These ensure that CRC can handle the installation)
** `homemade-serverless` is the name of the project where we'll be running this.  You can change this name if you'd like 
+
----
source scripts/shell-setup.sh
$DEMO_HOME/scripts/install-prereq.sh homemade-serverless --crc --skip-all-eventing
----
+
. Next use kbd:[CMD+p] to open `cr.yaml` file 
** Point out the timeout seconds as this will be important later
. Then apply this in the cluster
+
----
oc apply -f $DEMO_HOME/install/serverless/cr.yaml
----
+
. When that completes, then show the payment project in the explorer window and explain that this is a simple Quarkus project 
** Show the `pom.xml` file to show the libraries that go into compilation
** Explain how this is connecting to kafka outgoing for integration with payment topic
+
. Next, let's remove the aspects of the payment resource that causes it to poll the order service.  In vscode, use  to navigate to the *PaymentResource.java* file 
+
. Append a message to the COMPLETED message so that we can tell this service from others
+
image:payment-resource-string-change.png[]
+
. And delete the configuration for the incoming stream. In *application.properties* , delete (or comment out) the following lines for the Incoming stream:
+
image:payment-app-properties.png[]
+
** Explain how this is connecting to kafka outgoing for integration with payment topic
+
. Now rebuild the service locally
+
----
cd $DEMO_HOME/payment-service
export MAVEN_OPTS=" -Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled"
mvn clean package -DskipTests
----
+
. Now create a build configuration for OpenShift by running the following, but first:
** Explain that this just allows us to build and create an image that we store in the cluster using OpenShift specific functionality
** Explain that we could have just as easily built an image and pushed it up to some repository (which we'll point out later)
+
----
# Setup a binary based build for our quarkus instance
oc new-build  --image-stream="openshift/redhat-openjdk18-openshift:1.5" --binary --name=payment    
----
+
. And remotely (to upload the binary and bake it into a new image).  [blue]#NOTE: This should take about 1.5 minutes with crc cluster#
+
----
oc start-build payment --from-file target/*-runner.jar --follow 
----
+
. When the build is done, let's tag it as our initial revision
+
----
oc tag payment:latest payment:initial
----
+
. Next, show the image stream in the cluster by shifting to the [blue]#Browser# and shift-click on the _Administrator_ perspective.
+
. In that new tab, navigate to _Builds > ImageStreams_ and show that there is a new image in the image registry (reached from the _Administrator Perspective_ under _Builds > ImageStreams_):
+
image:payment-latest-image.png[]

==== Create Knative Serverless Service

. Now that we have our image tagged, let's create a knative service using that image.  
. First mention that we're using the knative CLI kn by issuing a `kn version` command
+
----
kn version
----
. We'll name the revision for the service name ({{.Service}} which will resolve to payment) and ({{.Generation}}, which should be 1)
+
[TIP]
.You can use the command line to quickly get the image stream
====
----
oc get is payment -o jsonpath="{.status.dockerImageRepository}" -n homemade-serverless
----
====
+
----
kn service create payment --image image-registry.openshift-image-registry.svc:5000/homemade-serverless/payment:initial --revision-name "{{.Service}}-{{.Generation}}"

# Which gives the output
Creating service 'payment' in namespace 'homemade-serverless':

  0.299s The Route is still working to reflect the latest desired specification.
  1.008s Configuration "payment" is waiting for a Revision to become ready.
 68.597s ...
 69.390s Ingress has not yet been reconciled.
 70.223s Ready to serve.

Service 'payment' created to latest revision 'payment-1' is available at URL:
http://payment.homemade-serverless.apps-crc.testing
----
. COPY the returned url (you'll need it in an upcoming part) especially if you've renamed the project that you're deploying to
+
** Show these aspects in the UI
+
image:knative-payment-revisions.png[]
+
** Show that the payment service is at 0 from the _Topology_ of the _Developer Perspective_
+
image:knative-developer.png[]
+
. Demonstrate that the service handling http requests invoking the service via curl
.. Open a [blue]#new terminal window (Watch Window)# that can be used to watch the payments topic and run this command
+
----
oc exec -c kafka my-cluster-kafka-0 -n homemade-serverless -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----
+
.. Make sure [blue]#Watch Window# is open and watching the payments topic
.. Make sure the [blue]#Browser# window with the _Developer Perspective_ can still be clearly seen
+
.. From the [blue]#Main shell# execute the following `curl` command
+
----
curl -i -H 'Content-Type: application/json' -X POST --data-binary @$DEMO_HOME/example/order-payload.json  http://payment.homemade-serverless.apps-crc.testing/
----
+
.. Show payment container spinning up
+
image:payment-spin-up.png[]
+
.. Show payment info being pushed to the queue
+
.. Then show it scaling back down to 0

== Demonstrate Knative Eventing (Remote Cluster)

[WARNING]
====
The coolstore and Knative Eventing require more horsepower than CRC can currently provide.  For this part of the demo you will need a separate external cluster running.

You can setup the coolstore by running the following commands after logging into the cluster
----
. scripts/shell-setup.sh
$DEMO_HOME/scripts/install-coolstore.sh -p coolstore
----

Wait until all the components have been installed.

_NOTE: You can cause cluster side rebuilds of all the components (instead of updating images to point to dockerhub) by using the `--rebuild` flag_
====

[red]#When you login to this cluster, be sure to record the context as remote by using this command#
----
oc config rename-context $(oc config current-context) remote-context
----

=== Screen and Window Setup

==== Screen 1

Here are how the windows should be laid out on Screen 1

. [blue]#Topology View#: A browser window with the _TopologyView_ of the _Developer Perspective_ open
. [blue]#Watch Window#: A new terminal windowthat can be used to watch the payments topic and runs this command:
+
----
oc exec -c kafka my-cluster-kafka-0 -n coolstore -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----
+
. [blue]#Coolstore#: A browser window that has the coolstore open so that an order can be placed
. Your windows should look something like this:
+
image:recommended-layout.png[]

==== Screen 2

On the other screen you should have a full screen view of VSCode, which you started from the `code .` command run from the _$DEMO_HOME_ directory.

=== Demonstrate the working Coolstore (without payment)

. First explain that we are now running on a separate cluster and that our coolstore with all its microservices are setup.
. Login to the OpenShift console, switch to the `coolstore` project and select the Developer Perspective and show all the different services 
+
image:.png[]
+
. Demonstrate the coolstore site working by kbd:[SHIFT] clicking the launcher icon on the Developer Perspective (see previous) and moving the newly opened window to the right 
** NOTE: you can also find the URL like this:
+
----
oc get route coolstore-ui -n coolstore
----
+
** [blue]#NOTE: You may find that it takes the page a while to load the first time, and also that the inventory might not show.  If this happens, just press reload#
+
image:add-to-cart.png[]
+
. From the cart, checkout and then enter credit card details (any 16-digit number beginning with 4 will work)
+
image:checkout.png[]
+
. Now navigate to the orders page.  Notice that the order gets filed but that it *doesn't* get processed
+
image:orders.png[]

=== Setup Knative Eventing

Now we want to use events the order topic to be our *source* (see also link:https://knative.dev/docs/eventing/samples/kafka/source/index.html[here] for generic example) and use the payment service as our *sink*

[WARNING]
====
The setup script should have handled this when setting up the coolstore project, but the Knative Eventing and Knative Kafka Eventing Operators should be installed on the cluster.  A good way to check this is to run this command in the `coolstore` project

----
oc get pods | grep -i ^kafka
----

You should see the following:
----
kafka-ch-controller-57cf94b477-dk9ss          1/1     Running     0          73s
kafka-controller-manager-56d58bb444-dtpkd     1/1     Running     0          81s
kafka-webhook-77b75f7c7f-df7vb                1/1     Running     0          72s
----
====

. Show all the installed operators
+
image:operators-all-necessary-installed.png[]
+
. Show the setup for the `Knative Eventing Kafka` by clicking on the highlighted link in the previous image, then clicking on the knative-eventing-kafka instance 
** Point the "bootstrapServers" in the resulting _Overview_
+
image:knative-eventing-kafka.png[]
+
. Now we create a simple event binding to the kafka event *source* to the payment service *sink*.  Use kbd:[CMD + p] to quickly open the `kafka-event-source.yaml`
+
image:kafka-event-source.png[]
+
. Apply that source to the cluster
+
----
oc apply -f $DEMO_HOME/payment-service/knative/kafka-event-source.yaml 
----
+
. Check to see if the event source is running.  It won't be running yet since the payment *sink* does not exist yet on this cluster
+
----
oc get pods -l eventing.knative.dev/SourceName=kafka-source-orders
----
+
. You can also refresh the orders page on the coolstore site and show that the payment is still not processed

=== Create a Payment Image on Coolstore Cluster

We need to find a way to get the image to our coolstore cluster.  Choose one of the following options to get it there:

. <<OPTION 1: Add the serverless payment service via skopeo,Copy from Destination Cluster>>
. <<OPTION 2: Add the serverless payment service via S2I,Build (native) image on cluster from S2I>>

===== OPTION 1: Add the serverless payment service via skopeo

[NOTE]
====
You will need to use kubernetes contexts to get this to work.  Use this command to list all the current contexts

----
oc config get-contexts
----

This will return a bunch of contexts that are defined.  You will want to find the NAME that is associated with your CRC cluster and store the whole of the name in `SRC_CLUSTER_CTX`.  Then find the remote cluster and store its NAME in `REMOTE_CLUSTER_CTX`.  If you marked your contexts as you logged into the different clusters this might look like this:

----
REMOTE_CLUSTER_CTX="remote-context"
SRC_CLUSTER_CTX="crc-context"
----
====

. Get the user and token from the coolstore (remote) cluster.  Assuming you are logged into the cluster with a token on the command line then issue the following commands
** [red]#NOTE: you can't use the password here.  It's a bearer token type login for the registry# 
** [red]#NOTE: the default login for the crc cluster is kube:admin, but the extra `:` confuses skopeo.  Thus we need to make sure to take out that `:` with `sed` before setting it as the src username#
+
----
oc config use-context $SRC_CLUSTER_CTX
SRC_CREDS="$(oc whoami | sed s/\://g):$(oc whoami -t)"
SRC_REPO="$(oc get route default-route -n openshift-image-registry -o jsonpath='{.spec.host}')"
oc config use-context $REMOTE_CLUSTER_CTX
REMOTE_CREDS="$(oc whoami):$(oc whoami -t)"
REMOTE_REPO="$(oc get route default-route -n openshift-image-registry -o jsonpath='{.spec.host}')"
----
+
. Next issue the `skopeo` command to copy the image from the src clusters image stream to the destination.
** NOTE: If there is difficulty, you can add a `--debug` just before the `copy` subcommand to see what's going on.  For instance, you may need to add a `src-` or `dest-` `tls-verify=false`
+
----
skopeo copy --src-creds ${SRC_CREDS} --src-tls-verify=false --dest-creds ${REMOTE_CREDS} docker://${SRC_REPO}/homemade-serverless/payment:initial docker://${REMOTE_REPO}/coolstore/payment:initial
----
+
[TIP]
====
If the image already exists on the cluster and you want to show again copying to the cluster, then you can do the following to remove the docker image layers:

. Remove any references to the image by removing image streams that point to it:
+
----
oc delete is payment
----
+
. Then, when you're sure there is nothing referencing the image in question, run this command (assuming `REMOTE_REPO` is still set from above)
+
----
oc adm prune images --registry-url=https://${REMOTE_REPO} --confirm
----
====
+
. Once the command completes, you should be able to navigate to the _Image Stream_ tab of the `coolstore` project in the destination cluster and see the image there
+
image:imagestream-payment-dest.png[]

==== OPTION 2: Add the serverless payment service via S2I

Let's create a quarkus native service to handle payment and use the power of the cluster to compile this

. Create a new Source to Image (S2I) build
+
----
oc new-build quay.io/quarkus/ubi-quarkus-native-s2i:19.2.0~https://github.com/hatmarch/serverless-demo.git --context-dir=payment-service --name=payment-native \
    -e MAVEN_OPTS=" -Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled"
oc cancel-build bc/payment-native
oc patch bc/payment-native -p '{"spec":{"resources":{"limits":{"cpu":"4", "memory":"6Gi"}}}}'
oc start-build bc/payment-native --follow
----
+
** Discuss that we're updating the build command to have more omph for building the native service
** whilst the command is running explain the different aspects of the command such as the builder image and the git repo reference
+
. Once the build has completed, tag the resulting image
+
----
oc tag payment-native:latest payment:initial-native
----
+
. You should now be able to see the image in the _ImageStream_ for payment in the `coolstore`

=== Create Payment Knative Service (on Remote)

Once you have a `payment:initial` image in the `coolstore` we need to add the service

. Next create a new knative payment knative service (as we did previously on the CRC instance)
** NOTE: the -l flag is a label that will allow the service to show up as part of the "focus" topology
** TIP: if you want to set the concurrency limit per revision, you can use the `--concurrency-limit=2` flag
+
----
kn service create payment --image image-registry.openshift-image-registry.svc:5000/coolstore/payment:initial --revision-name "{{.Service}}-{{.Generation}}" -l app.kubernetes.io/part-of=focus 
----
+
. Demonstrate the the service is ready (and dormant) by showing the topology view
+
image:topology-coolstore-payment.png[]
+
. Show that the `kafka-source-orders` has also spun up.  You can do this either by viewing the _Topology View_ of the _Developer Perspective_ or by issuing the following command:
+
----
oc get pods -l eventing.knative.dev/SourceName=kafka-source-orders
----
+
. You might also show that the order we had in our queue has now been processed

=== Demonstrate event driven serverless

. First make sure your screens are arranged as suggested in <<Screen and Window Setup,Screen and Window Setup>>
. Next, use the coolstore site to order something
+
image:checkout.png[]
+
. Upon checkout you should see the payment pod spinning up to consume the order in the [blue]#Watch Window#
+
image:consuming-kafka-queue.png[]
+
. You can then go to the *Orders* section of the site to show that the order was consumed.  When you return to the [blue]#Topology View# the pod should be spun down (with a clear or black outline).
+
. Attempt to make a second order before the service spins down, notice that it's processed immediate
+
. Finally watch the topology view until the service spins down to nothing. 

=== Demonstrating Concurrency

. Show the concurrency limit on the service by selecting the _KSVC_ in the Topology view and selecting the revision
** ALTERNATIVELY: if you have not set this on a per revision basis, you can show the global setting the knative instance
+
image:knative-revision-concurrency.png[]
+
. Open a new tab with the _Topology View_ of the _Developer Perspective_
. Run the following `hey` command to show the payment service running under load
** NOTE: information on the `hey` command can be found link:https://github.com/rakyll/hey[here]
+
----
hey -n 100 -c 100 -m POST -D $DEMO_HOME/example/order-payload.json -T "application/json" $(oc get rt payment -o jsonpath='{.status.url}')
----
+
. Navigate to the Topology view while the command is running:
** Notice number of containers that are spun up, this should be <NUM_REQUESTS>/<MAX_CONCURRENCY>
. When the `hey` command report comes back:
** Notice the timings
+
image:hey-example-timings.png[Example timings]


=== Demonstrating Knative Revisions (featuring native Quarkus)

[WARNING]
====
If you are running linux in a container, you need to make sure the docker daemon has enough memory assigned to it, otherwise the native quarkus build will fail towards the end of the run.  This configuration seemed to be enough to build the payment-service:

image:docker-requirement.png[]
====

. Use kbd:[CMD+p] to quickly open the `PaymentResource.java` and update the _COMPLETED_ message in the `pass` function:
+
image:payment-completed-log.png[]
+
. Next, build a native image (locally).
** If you would like to build the image using S2I, you'll need to first checkin the changes and see <<,these instructions>>
+
----
cd payment-service
export MAVEN_OPTS=" -Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled"
mvn clean package -Pnative -DskipTests
----
+
. Next, add a build to our project that will allow us to create an image out of the binary we just compiled.
+
----
oc new-build quay.io/quarkus/ubi-quarkus-native-binary-s2i:19.2.0 --binary --name=payment-native
----
+
Next, start the (binary) build of the image using our new configuration
+
----
oc start-build payment-native --from-file $DEMO_HOME/payment-service/target/*-runner --follow
----
+
. When finished, then tag this latest build as a `payment:quarkus-native` build
+
----
oc tag payment-native:latest payment:quarkus-native
----
+
. Next, update our payment knative service to use the quarkus-native image we just created (keeping concurrency limits the same)
** NOTE: if you don't want to write out the location to the image registry, you can use this embedded oc command after the `--image` switch
+
----
oc get istag/payment:quarkus-native -o jsonpath='{.image.dockerImageReference}'
----
+
----
kn service update payment --image $(oc get is/payment -o jsonpath='{.status.dockerImageRepository}'):quarkus-native --revision-name "{{.Service}}-{{.Generation}}"
----
. Show revisions in developer console
+
image:knative-revisions.png[]
+
. Run the following `hey` command to show the payment service running under load
** NOTE: information on the `hey` command can be found link:https://github.com/rakyll/hey[here]
+
----
hey -n 100 -c 100 -m POST -D $DEMO_HOME/example/order-payload.json -T "application/json" $(oc get rt payment -o jsonpath='{.status.url}')
----
+
. Navigate to the Topology view while the command is running:
** Notice number of containers that are spun up, this should be <NUM_REQUESTS>/<MAX_CONCURRENCY>
. When the `hey` command report comes back:
** Notice the timings
+
image:hey-example-timings-quarkus.png[Example timings with quarkus native]
+
. Update the traffic in the _Topology View_ back to the initial revision as per the instructions in the screenshot
+
image:knative-update-traffic-distrubtion.png[]
+
. Run `hey` again
. Switch back to the _Topology View_ and notice that revision 1 is getting the traffic
+
image:knative-back-to-initial-revision.png[]
+
. Look back at the `hey` results
** Notice that the timings are now back in line with the initial revision


== Troubleshooting ==

=== 500 errors

You may notice 500 errors, particularly if you send multiple requests under load:

image:500-errors.png[]

I believe this is because there is currently a race condition when the second request hits a pod where the payment topic (`producer` in the code) is not fully setup in the payment service (thus a null pointer).  Looks like the first exception happens in the `pass` function but this is caught in the handleCloudEvent function, only for the `fail` event to use the `producer` null pointer to try to log a failure at which time a new uncaught exception is raised.

If you set the concurrently limit to 1, you should be able to demonstration that this error doesn't happen with hey

=== Getting logs of Knative service

The epheral nature of the knative service can make it hard to capture logs of the service, particularly if you notice that the service had issues after it's gone.

Aside from setting up Elasticsearch to retain all logs, you can consider using `stern` in the background.  Using the .devcontainer that is run from within VSCode, you can have the following command running in a background terminal:

----
stern -l serving.knative.dev/service=payment
----

To see all the logs from revision 1 of the payment service (-1 represents the revision number I believe).  This command will include logs from all containers associated with the pod (such as `queue-proxy`).  If you only want the deployed code itself to log, add the `-c user-container` flag

=== Viewing and Modifying Order (MongoDB) Database

You cannot connect to the mongodb instance using the latest plain adminer container.  Instead you need to follow the special instructions below.  If you my version of adminer does not work for you, you can attempt to follow <<Updating your own Adminer image,these instructions>> for creating a new image yourself from the latest.

. Start port forwarding to the mongodb service
+
----
oc port-forward -n coolstore svc/order-database 27017:27017
----
+
. Run the modified adminer pod
** NOTE: `quay.io/mhildenb/myadminer:1.1` is a version 4.7.6 adminer container that I've updated to support this
+
----
docker run -p 8080:8080 -e ADMINER_DEFAULT_SERVER=docker.for.mac.localhost quay.io/mhildenb/myadminer:1.1
----
+
. Login as shown
+
image:adminer-mongo-password.png[]
+
. You should now have access to the mongo database with the ability to list and edit entries:
+
image:adminer-mongo-edit.png[]

==== Updating your own Adminer image

There are two reasons why the normal adminer image cannot connect to the mongo database:

1. It requires a newer version of php integration with MongoDB
2. The mongoDB is not setup with a user and a password (Adminer does not allow access to such databases by default for security reasons)

To update the latest adminer image to be able to connect to the userless MongoDB follow these instructions:

. Run an instance of the adminer container as follows:
+
----
docker run -it -u root --name my_adminer adminer:latest sh 
----
** NOTE: If an instance of the container is already running you can use the `docker exec -it` command instead
+
. Then from inside the container run
+
----
apk add autoconf gcc g++ make libffi-dev openssl-dev
pecl install mongodb
echo "extension=mongodb.so" > /usr/local/etc/php/conf.d/docker-php-ext-mongodb.ini
----
+
. Next add a plugin as per link:https://nerdpress.org/2019/10/23/adminer-for-sqlite-in-docker/[This site].  It will require you to create a login-password-less.php file in the `/var/www/html/plugins-enabled/` directory
+
[CONTENTS]
====
----
<?php
require_once('plugins/login-password-less.php');

/** Set allowed password
 * @param string result of password_hash
 */
return new AdminerLoginPasswordLess(
    $password_hash = password_hash("admin", PASSWORD_DEFAULT)
);
----
====
+
. now commit this container as a new image
+
----
docker commit my_adminer myadminer:1.1    
----

=== Insecure ImageRegistry ===

Might be solved as per link:https://github.com/knative/serving/issues/2136[here] but can't get the controller pod to take the new environment variable

Looks like it has something to do with the labels.  If the sha is used instead it seems to work properly.  You can find the sha like this:
----
$ oc get istag/payment:latest -o jsonpath='{.image.dockerImageReference}'
image-registry.openshift-image-registry.svc:5000/user1-cloudnativeapps/payment@sha256:21ca1acc3f292b6e94fab82fe7a9cf7ff743e4a8c9459f711ffad125379cf3c7
----

And then apply it as a service like this:
----
kn service create payment --image $(oc get istag/payment:initial-build -o jsonpath='{.image.dockerImageReference}') --label "app.kubernetes.io/part-of=focus" --revision-name "{{.Service}}-{{.Generation}}" --annotation sidecar.istio.io/inject=false --force
----

----
oc port-forward <image-registry-pod> -n openshift-image-registry 5001:5000
----

To get the cert as a pem file, do this:
----
openssl s_client -showcerts -connect localhost:5001 </dev/null 2>/dev/null|openssl x509 -outform PEM >mycertfile.pem
----

== Appendix

=== Copying OpenShift images to public repositories

If you have images that you've compiled on an OpenShift cluster and you want to pull them out of the local image stream to something like `quay.io`, you can use one of the following approaches to copy the images out of openshift.  Both use the `skopeo` command which is installed by default in the .devcontainer.  

For both examples, it assumes the copying of a payment service.  As such, note the following for the different variables:

* USER: your username for the public repository
* PASSWORD: your password or TOKEN for the public repository
* PROJECT: the project your image stream lives in (such as coolstore)
* IMAGE_DEST: Replace this with your repository, project, image-name, and version, example: `quay.io/mhildenb/homemade-serverless-java:1.0`: 

==== Image Registry is exposed publicly 

You need only run the following command:

----
skopeo copy --src-creds "$(oc whoami):$(oc whoami -t)" --dest-creds "${USER}:${PASSWORD}" docker://$(oc get is payment -o jsonpath='{.status.publicDockerImageRepository}'):latest docker://{IMAGE_DEST}       
----

==== Image Registry is private

If instead you need to copy from an image registry that is not exposed outside the cluster, you must instead do the following:

. Port forward to openshift's internal image registry
+
----
oc port-forward svc/image-registry -n openshift-image-registry 5000:5000
----
+
. Then in a separate shell, run the following command
+
----
skopeo copy --src-creds "$(oc whoami):$(oc whoami -t)" --src-tls-verify=false --dest-creds "${USER}:${PASSWORD}" docker://localhost:5000/${PROJECT}/payment:latest docker://{IMAGE_DEST}
----

=== Install the initial payment MIRCO service 

This section is necessary if you're wanting to show the conv

. Run the following commands in a shell
+
----
. scripts/shell-setup.sh
code .
----
+
. From a shell in VSCode, run the following to install the necessary prerequisites (NOTE the `--crc` flag)

+
----
. scripts/shell-setup.sh
$DEMO_HOME/scripts/install-prereq.sh homemade-serverless --crc 
----
+
. When that completes, then install the payment service
+
----
$DEMO_HOME/scripts/install-payment.sh
----
+
. Next use the crc _Developer Perspective_ and _Topology_ to show what is currently in our project
+
image:developer-payment-alone.png[]
+
.. Explain that the payment service will watch the orders topic and "process that" and put the output on the payments topic
.. Show that there is one instance of the payment service running all the time
.. Show the different kafka nodes
+
. Next demonstrate how the payment service currently interacts with the kafka queues by setting up two windows
+
.. *Terminal Window 2* Run the following command to watch the payments:
+
----
oc exec -c kafka my-cluster-kafka-0 -n user1-cloudnativeapps -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----
+
.. *Terminal Window 1* Run the following command to simulate an order being placed by the coolstore
+
----
cat $DEMO_HOME/example/order-payload.json | oc exec -i -c kafka my-cluster-kafka-0 -n user1-cloudnativeapps -- /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic orders
----
+
. You should now see the order being processed as seen here:
+
image:payment-kafka-test.png[]

=== Convert Payment Service to Serverless 

Now lets wrap our payment service in a knative service.  This will allow knative to manage the container and decide when new containers should be started.  In wrapping it in a service, we're expecting it to no longer need to poll the kafka topic.

. First stop our payment service from being run all the time (by deleting a deployment) and remove all connections to it
+
----
oc delete dc/payment route/payment svc/payment
----
+
. Next, let's remove the aspects of the payment resource that causes it to poll the order service.  In vscode, use kbd:[CMD+p] to navigate to the *PaymentResource.java* file 
+
. Delete (or comment out) the onMessage() method:
+
image:onMessage.png[]
+
. And delete the configuration for the incoming stream. In *application.properties* , delete (or comment out) the following lines for the Incoming stream:
+
image:payment-app-properties.png[]
+
** Explain that this is no longer necessary because instead the event will trigger the starting of a container with the event as the incoming context.
+
. Now rebuild the service locally
+
----
cd $DEMO_HOME/payment-service
export MAVEN_OPTS=" -Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M \
  -XX:+CMSClassUnloadingEnabled"
mvn clean package -DskipTests
----
+
. And remotely (to upload the binary and bake it into a new image).  [blue]#NOTE: This should take about 1.5 minutes with crc cluster#
+
----
oc start-build payment --from-file target/*-runner.jar --follow 
----
+
** When the build is done, notice that there is a new image in the image registry (reached from the _Administrator Perspective_ under _Builds > ImageStreams_):
+
image:payment-latest-image.png[]
+
. Now we want to specially tag this image as not using kafka
+
----
oc tag payment:latest payment:nokafka
----
+
. Next let's create a knative service using the image we just tagged.  We'll name the revision for the service name ({{.Service}} which will resolve to payment) and ({{.Generation}}, which should be 1)
+
[TIP]
.You can use the command line to quickly get the image stream
====
----
oc get is payment -o jsonpath="{.status.dockerImageRepository}" -n user1-cloudnativeapps
----
====
+
----
kn service create payment --image image-registry.openshift-image-registry.svc:5000/user1-cloudnativeapps/payment:nokafka --revision-name "{{.Service}}-{{.Generation}}"

# Which gives the output
Creating service 'payment' in namespace 'user1-cloudnativeapps':

  0.299s The Route is still working to reflect the latest desired specification.
  1.008s Configuration "payment" is waiting for a Revision to become ready.
 68.597s ...
 69.390s Ingress has not yet been reconciled.
 70.223s Ready to serve.

Service 'payment' created to latest revision 'payment-1' is available at URL:
http://payment.user1-cloudnativeapps.apps-crc.testing
----
+
** Show these aspects in the UI
+
image:knative-payment-revisions.png[]
+
** Show that the payment service is at 0 from the _Topology_ of the _Developer Perspective_
+
image:knative-developer.png[]
+
. Demonstrate that the service handling http requests by invoking using the RESTClient extension in vscode.  
.. Make sure *Terminal Window 2* (from above) is still open and watching the payments topic.
.. Use kbd:[CMD+p] and enter `payment.http` to open this file quickly
.. Click the "code-lens" above the first instance to post to the service
+
image:payment-restclient.png[]
+
.. Switch quickly to the _Developer Perspective_ to show the service spinning up
+
image:payment-spin-up.png[]
+
.. Switch back to the shell showing the kafka queue
+
.. Then show it scaling back down to 0


=== Update Payment Service 

. Edit the PaymentResource so that the success output shows some sort of change

. Compile locally with the following command
+
----
cd $DEMO_HOME/payment-service
mvn package -DskipTests
----
+
. When the compilation is done, upload the binary to make a new image ([blue]#NOTE: This should take about 1.5 minutes with crc cluster)#
+
----
oc start-build payment --from-file target/*-runner.jar --follow
----
+
. Then update the service with the newest revision
+
----
kn service update payment --image $(oc get istag/payment:latest -o jsonpath='{.image.dockerImageReference}') --revision-name "{{.Service}}-{{.Generation}}"
----
+
. Demonstrate that there is a new revision that is taking traffic
+
----
$ kn service describe payment
Name:       payment
Namespace:  user1-cloudnativeapps
Age:        4h
URL:        http://payment.user1-cloudnativeapps.apps-crc.testing

Revisions:  
  100%  @latest (payment-5) [5] (29s)
        Image:  image-registry.openshift-image-registry.svc:5000/user1-cloudnativeapps/payment:latest (pinned to 64a5a8)

Conditions:  
  OK TYPE                   AGE REASON
  ++ Ready                  20s 
  ++ ConfigurationsReady    21s 
  ++ RoutesReady            20s 
----
+
** the _pinned to_ field should match the first characters of the sha for the image labelled `:latest`
+
image:find-image.png[]


=== Deleting Knative Services

Though knative services are reported from oc get svc and oc get rt, you cannot delete them in this way.  Instead you must delete them based on the distinct (knative) descriptor that they have

Here is a command to delete all services (exposed as routes) that have the name that includes 'payment'
----
oc delete services.serving.knative.dev $(oc get rt --no-headers | grep -i payment | awk '{print $1}')
----

=== Alternative Knative service creation (without `kn` client)

Knative Services can be created not just using the command line but also by creating resources using yaml

----
oc apply -f payment-service/knative/knative-serving-service.yaml 
----

It will take a while for ingress to be configured for the service.  You can issue this command and you should see the following output:

----
$ watch oc get rt
NAME      URL                                                                           READY   REASON
payment   http://payment.user1-cloudnativeapps.apps.service-mesh-demo.openshifttc.com   True    
----

=== Trigger Knative Service with Kafka Event

Now we want to use events the order topic to be our *source* (see also link:https://knative.dev/docs/eventing/samples/kafka/source/index.html[here] for generic example) and use the payment service as our *sink*

[NOTE]
====
First we need to install the Kafka knative event source operator as seen below.  Go to link:demo-setup.adoc[here] for more details

image:kafka-event-operator.png[]
====

. Create an instance of kakfa eventing for our namespace.  To do this call
+
----
$ oc apply -f $DEMO_HOME/install/kafka-eventing/kafka-eventing.yaml 
knativeeventingkafka.eventing.knative.dev/knative-eventing-kafka created

$ oc wait --for=condition=InstallSucceeded KnativeEventingKafka knative-eventing-kafka
----
+
.. When the command completes, the following pods will be able to be seen (with the following command):
+
----
$ watch "oc get pods | grep -i ^kafka"

kafka-ch-controller-57cf94b477-dk9ss          1/1     Running     0          73s
kafka-controller-manager-56d58bb444-dtpkd     1/1     Running     0          81s
kafka-webhook-77b75f7c7f-df7vb                1/1     Running     0          72s
----
+
. Then use the yaml file to bind the kafka event source to the payment service sink (show what's going on first)
+
image:kafka-event-source.png[]
+
----
oc apply -f $DEMO_HOME/payment-service/knative/kafka-event-source.yaml 
----
+
. Check to make source the event source is running
+
----
$ oc get pods -l knative-eventing-source-name=kafka-source
----
+
. Demonstrate event driven serverless
+
.. Make sure the _Developer Perspective_ can be seen
.. Have *Terminal Window 2* up showing what gets written to the payment queue
.. In *Terminal Window 1* run the following command to simulate an order coming in from the order service
+
----
cat $DEMO_HOME/example/order-payload.json | oc exec -i -c kafka my-cluster-kafka-0 -n user1-cloudnativeapps -- /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic orders
----
+
.. Show that the service gets spun up and show the results of the processing in the payment queue

== Knative payment on full cluster (integration with website) 

=== Install the coolstore project 

WARNING: You will need to log into a real OpenShift cluster to run the coolstore website.  The requirements are too heavy to run it with crc (as of verion 1.8)

. Locally log into the cluster with `oc login` command
. Setup local environment
+
----
source scripts/shell-setup.sh
----
+
. Run the coolstore setup script. NOTE: You can cause cluster side rebuilds of all the components (instead of updating images to point to dockerhub) by using the `--rebuild` flag
+
----
$DEMO_HOME/scripts/install-coolstore.sh -p coolstore
----
+
. It will take a little while for all the pods to be deployed (and images downloaded)
. Test the deployment by getting the route
+
----
echo "http://$(oc get route coolstore-ui -o jsonpath='{.spec.host}')/"
----

=== Building the Payment Service

This command does not build the payment service as it's assumed that the it will get changed.  But if you need to create the payment service

----
cd $DEMO_HOME/payment-service
export MAVEN_OPTS="-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled"
mvn clean package -DskipTests
----

When the build is done, deploy with:

----
oc start-build payment --from-file target/*-runner.jar --follow
----

Finally, be sure to tag the payment build (we'll need this for service revisions later)
----
oc tag payment:latest payment:original
----

=== Remove direct Knative integration code from original payment service

Currently our Payment service directly binds to Kafka to listen for events. Now that we have Knative eventing integration, we no longer need this code. CMD+p to navigate to the *PaymentResource.java* file 

Delete (or comment out) the onMessage() method:

image:onMessage.png[]

And delete the configuration for the incoming stream. In *application.properties* , delete (or comment out) the following lines for the Incoming stream:

image:payment-app-properties.png[]

Explain that this is no longer necessary because instead the event will trigger the starting of a container with the event as the incoming context.

Now rebuild the service locally
----
cd $DEMO_HOME/payment-service
export MAVEN_OPTS=" -Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled"
mvn clean package -DskipTests
----

And remotely

----
oc start-build payment --from-file target/*-runner.jar --follow
----

Then tag this new image

----
oc tag payment:latest payment:noqueue
----

And update our revision to use the image we just tagged.  We'll name the revision for the service name ({{.Service}} which will resolve to payment) and ({{.Generation}}, which should be 2)

----
kn service update payment --image $(oc get istag/payment:noqueue -o jsonpath='{.image.dockerImageReference}') --revision-name "{{.Service}}-{{.Generation}}"
----

Demonstrate that there is a new revision that is taking traffic

----
$ kn service describe payment
Name:         payment
Namespace:    user1-cloudnativeapps
Labels:       app.kubernetes.io/part-of=focus
Annotations:  sidecar.istio.io/inject=false
Age:          36m
URL:          http://payment.user1-cloudnativeapps.apps.service-mesh-demo.openshifttc.com
Address:      http://payment.user1-cloudnativeapps.svc.cluster.local

Revisions:  
  100%  @latest (payment-3) [3] (1m)
        Image:  image-registry.openshift-image-registry.svc:5000/user1-cloudnativeapps/payment:noqueue (pinned to 21ca1a)

Conditions:  
  OK TYPE                   AGE REASON
  ++ Ready                   1m 
  ++ ConfigurationsReady     1m 
  ++ RoutesReady             1m 
----

=== Recompile the payment service (native quarkus)

[WARNING]
.Resource Requirements for Docker Quarkus Build
====
If you are running linux in a container, you need to make sure the docker daemon has enough memory assigned to it.  This configuration seemed to be enough to build the payment-service

image:docker-requirement.png[]
====

----
cd payment-service
export MAVEN_OPTS=" -Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled"
mvn clean package -Pnative -DskipTests
----

While that's compiling, in another VSCode terminal, update our builder to be able to build native quarkus

----
oc delete bc payment
oc new-build quay.io/quarkus/ubi-quarkus-native-binary-s2i:19.2.0 --binary --name=payment -l app=payment
----

Once the native maven build is done, we can start a build using our new configuration

----
cd $DEMO_HOME/payment-service
oc start-build payment --from-file target/*-runner --follow
----

Then tag this latest build accordingly

----
oc tag payment:latest payment:quarkus-native
----

Then update our knative service (tagging revisions before and after updating the service)

----
kn service update payment --tag @latest=traditional
kn service update payment --image $(oc get istag/payment:quarkus-native -o jsonpath='{.image.dockerImageReference}') --revision-name "{{.Service}}-{{.Generation}}"
kn service update payment --tag @latest=native
----

=== Knative Revisions

Start with traffic to the original version

----
$ kn service update payment --traffic traditional=100
Updating Service 'payment' in namespace 'user1-cloudnativeapps':

  0.275s Ingress has not yet been reconciled.
  1.401s Ready to serve.

Service 'payment' updated with latest revision 'payment-4' (unchanged) and URL:
http://payment.user1-cloudnativeapps.apps.cluster-nab-92c5.nab-92c5.example.opentlc.com

export SVC_URL=$(oc get rt payment -o template='{{ .status.url }}')

$ curl $SVC_URL -s -o /dev/null -w "%{time_starttransfer}\n"
12.305205
$ curl $SVC_URL -s -o /dev/null -w "%{time_starttransfer}\n"
0.546286
----

Now look at the quarkus native version

----
$ kn service update payment --traffic native=100
export SVC_URL=$(oc get rt payment -o template='{{ .status.url }}')

$ curl $SVC_URL -s -o /dev/null -w "%{time_starttransfer}\n"f 
10.930526
$ curl $SVC_URL -s -o /dev/null -w "%{time_starttransfer}\n"
0.543870
----

TODO: See link:https://blog.openshift.com/knative-configurations-routes-and-revisions/[here]

TODO: See link:https://redhat-developer-demos.github.io/knative-tutorial/knative-tutorial-basics/0.7.x/02-basic-fundas.html#deploying-new-revision[here]

Tagging in ImageStream
----
oc tag payment@sha256:573f369a858c692b71f02acb470b321816d8ff8ababece8148ac8c939a37c9e2 payment:java
----

=== Service Autoscaling

NOTE: _The knative-serving attribute scale-to-zero-grace-period is a “dynamic parameter” i.e. any updates to this value are reflected immediately to all its consumers; while all other parameters are static parameters i.e. change to it need a restart of the autoscaler deployment of knative-serving namespace._

TODO: See link:https://knative.dev/docs/serving/samples/autoscale-go/index.html[here]
Then reopen the website

=== Service Pinning

TODO: See link:https://redhat-developer-demos.github.io/knative-tutorial/knative-tutorial-basics/0.7.x/02-basic-fundas.html#_service_pinned_to_first_revision[here]
