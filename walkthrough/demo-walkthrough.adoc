= Serverless Demo 
:experimental:
:imagesdir: images
:toc:
:toclevels: 4

[IMPORTANT]
.On necessary operators
====
See link:demo-setup.adoc[] for information on operators and other prerequisites that must be installed for the demo to run properly.
====

This demo centers around the conversion of a traditional (payment) service into one that is implemented serverless-ly.  It supports the "Coolstore" website the relevant architecture of which can be seen here:

image:coolstore-arch.png[]

== Setup Tips Prior to Walkthrough ==

* [OPTIONAL] Open CodeReadyWorkspaces with the devfile to ensure it is initialized at least once
** _This demo has image pre-pull support so the startup time should not be so bad_
* VSCode in the Desktop to the right of the main demo desktop


=== Desktops

==== Desktop 0

This should be offscreen

. *Hidden Shell*: This is the shell for running support commands that would otherwise confuse the demo
.. Go to the root of the `serverless-demo` directory on your local machine
.. Run the same docker command from before
+
----
docker run --rm -it -v ~/.kube:/home/jboss/.kube -v /var/run/docker.sock.raw:/var/run/docker.sock -v ~/.oh-my-zsh:/home/jboss/.oh-my-zsh -v $(pwd):/workspaces/serverless-demo -w /workspaces/serverless-demo quay.io/mhildenb/kn-demo-shell /bin/zsh
----
+
.. Ensure you are logged in as a `cluster-admin`

==== Desktop 1 (Main)

. *Shell 1*: This will be the main shell for commands to run
.. Go to the root of the `serverless-demo` directory on your local machine
.. Run this docker command from that directory (so that all the remaining commands will be run from inside a container will all the necessary tools installed)
+
----
docker run --rm -it -v ~/.kube:/home/jboss/.kube -v /var/run/docker.sock.raw:/var/run/docker.sock -v ~/.oh-my-zsh:/home/jboss/.oh-my-zsh -v $(pwd):/workspaces/serverless-demo -w /workspaces/serverless-demo quay.io/mhildenb/kn-demo-shell /bin/zsh
----
+
.. Ensure you are logged in as a `cluster-admin`
. *Window 1*: Chrome with the following tabs
.. Topology view of the Dev Project
.. Cluster's Gitea home (logged in)
. *Window 2*: Have a second window with the following tabs
.. Pipelines view of the kn-demo-cicd project to look for pipeline runs
+
. *Kafka Shell*: This will be used to show what's coming through the payment queue
.. Go to the root of the `serverless-demo` directory on your local machine
.. Run this docker command from that directory (so that all the remaining commands will be run from inside a container will all the necessary tools installed)
+
----
docker run --rm -it -v ~/.kube:/home/jboss/.kube -v /var/run/docker.sock.raw:/var/run/docker.sock -v ~/.oh-my-zsh:/home/jboss/.oh-my-zsh -v $(pwd):/workspaces/serverless-demo -w /workspaces/serverless-demo quay.io/mhildenb/kn-demo-shell /bin/zsh
----
+
.. Ensure you are logged in as a `cluster-admin`
.. Run this command to watch the queue
+
----
 oc exec -c kafka my-cluster-kafka-0 -n $dev_prj -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----

==== Desktop 2 (VSCode)

. Have VSCode opened (remote) at the root of the `serverless-demo` directory.

== Intro to Coolstore

. Open the Topology View
.. Notice all the different support services
.. Be sure to point out the *Kafka Cluster*
. Set to `Focus`
.. Now just on the services running
.. Point out that the payment service is running just like the rest
. Set up watches on the different kafka topics
.. In the `Kakfa Order Shell` run the following command:
+
----
oc exec -c kafka my-cluster-kafka-0 -n $dev_prj -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic orders
----
+
.. In the `Kafka Payment Shell` run the following command:
+
----
oc exec -c kafka my-cluster-kafka-0 -n $dev_prj -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----
+
. Click the route badge to open the website
. Buy something 
. You should see the order and the payment come through the shells
. In the website, click on Orders.  You should see the following:
+
image:initial-order-purchase.png[]


== Installing OpenShift Serverless

. In a new tab, go to Operator Hub
. Search for `Serverless`
+
image:operator-hub-serverless.png[]
+
.. Show the information about the operator
. Install the operator with all the defaults
+
image:operator-defaults.png[]
+
. Wait until operator is installed and then click on the "View Operator" button or link
+
image:operator-wait.png[]
+
. Switch to `knative-serving` project (which was automatically created by the operator)
+
image:operator-switch.png[]
+
. Create the `KnativeServing` instance for the cluster by clicking on the highlighted link
+
image:start-knative-serving.png[]
+
. Once on the page listing all the `Knative Servings` for the knative-serving project, click the "Create Knative Serving" link
.. Show different options in `Form View` and explain how this govens the defaults for how serverless deployments will behave
. Switch to `YAML View` and paste contents from link:../install/serverless/cr.yaml[the cr.yaml file in the project]
*** You can use kbd:[CMD + P] to quickly open `cr.yaml` in VS Code and copy all the contents
+
image:knative-cr-interesting.png[]
+
. Click `Create` button at bottom of page
. Point out that the `Serverless` drawer now appears in the UI
.. There's not much to see yet, but you might click into  it
.. Talk about OpenShift is tightly integrated with OpenShift Serving and new options are available all over the cluster, as will be seen in the next section
. Before moving onto the next section, make sure knative-serving is ready by typing in the `shell`
+
----
oc wait --for=condition=InstallSucceeded knativeserving/knative-serving --timeout=6m -n knative-serving
----
+
.. When knative-serving is fully installed, the command will return with:
+
----
knativeserving.operator.knative.dev/knative-serving condition met
----

== Creating Knative Service

. Ensure that the `Topology View` of the `kn-demo-dev` project is visible in the `Browser Window`
+
image:topology-view.png[]
+
. Set the payment-traditional service down to 0
.. If you want to do this from the `shell` instead:
+
----
oc scale --replicas=0 deployment/payment-traditional -n $dev_prj
----
+
. To prove that nothing is addressing orders as they come in, run the following command in the `shell` to simulate the placement of an order:
+
----
cat $DEMO_HOME/example/order-payload.json | oc exec -i -c kafka my-cluster-kafka-0 -n $dev_prj -- /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic orders
----
+
.. You should see an order in the `Kafka Order Shell` but no corresponding payment processing in the `Kafka Payments Shell`
.. You can also navigate to the orders page of the Coolstore website and see the order not processed
+
image:unprocessed-order.png[]
+
. Next right-click on the `focus` grouping to add a new application from git
+
image:add-kn-service-git.png[]
+
. Get the repo URL from the following command
+
----
echo "https://$(oc get route gitea -n $cicd_prj -o jsonpath='{.spec.host}')/gitea/coolstore"
----
+
. Fill in the form with the following:
** _Git Repo URL_: $REPO
** _Git Reference_ (advanced options): serverless-demo
** _Builder_: Java, `openjdk-11-ubi8`
** _Name_: payment
** _Resources_: Knative Service
** _Pipelines_: Do not add Pipeline
** _Build Configuration_ (advanced options): Uncheck all
** _Scaling_ (advanced options): 
*** _Concurrency Target_: 1
*** _Concurrency Limit_: 1
+
image:import-from-git-1.png[]
image:import-from-git-2.png[]
image:import-from-git-3.png[]
image:import-from-git-4.png[]
+
. In the `hidden shell` run the following command to create and annotate the revision so that we get a badge for CRW
+
----
kn service update payment -n $dev_prj --image quay.io/rhdevelopers/knative-tutorial-greeter:quarkus --annotation "app.openshift.io/vcs-ref=serverless-demo" --annotation "app.openshift.io/vcs-uri=https://github.com/hatmarch/coolstore.git" --revision-name "{{.Service}}-initial"
----
. When the revision comes up you will see the badge for editing the code in CRW
+
image:initial-revision-badge.png[]

== Turn payment service "Serverless" in CodeReady Workspaces

[IMPORTANT]
.Seeding the CRW
====
The workspace needs to download a lot of maven packages to get the language server to run.  You can cut down on this time if you run the "Seed Cache" command earlier in the process than the instructions indicate.

If you find yourself waiting around for the the "Activating Quarkus" prompt to go away, then this might speed things up

image:activating-quarkus.png[]

====

. Click on the CRW badge that is on the new payment knative service Revision
.. This will launch CRW.  It will take a little while to load the workspace from the Devfile
. While waiting, you could show the `Devfile` in VSCode (use kbd:[CMD + p] to quickly open `devfile`)
. Once in CRW, quickly open the `PaymentResource` by using kbd:[CMD+p]
.. Show where the PaymentResouce is in the project
.. Mention that it's written in Quarkus
.. Explain
*** Emitter (and kbd:[CMD + p] `application.properties`) to show integration with Kafka
*** `onMessage`: This is called from watching for incoming orders
*** `handleCloudEvent`: This is called when orders are found
*** `pass`, `fail`, `createPayment`: These functions create a payload for the emitter
. Edit the PaymentResource such that it looks like the diff on the right
** *NOTE:* You can also copy and paste from link:../example/payment-knative/PaymentResource.java[example/payment-knative/PaymentResource.java]
+
image:knative-diffs.png[]
+
. Open the side bar to and select `Seed Cache` to ensure a quicker local compile
. Then select `New Terminal` to open a terminal
+
image:crw-sidebar-terminal.png[]
+
. In the terminal, run this command to run in live mode
+
----
cd coolstore/payment-service
mvn quarkus:dev
----
+
. When the code starts running, CRW will prompt you that different routes and ports are open
.. When the main route comes up, copy this route
** [red]#NOTE:# You may have to refresh the page a couple times before it comes up
+
image:preview-route.png[]
+
. In the `shell`, set the CRW_ROUTE variable with the route that was published by CRW
+
----
CRW_ROUTE=<COPIED ROUTE>
----
+
. Access the route by posting to it with this command
+
----
curl -i -H 'Content-Type: application/json' -X POST -d @$DEMO_HOME/example/order-payload.json $CRW_ROUTE
----
+
. You should now see something posted to the `Kafka Payments Shell` with a tell-tale change of:
+
----
... "status":"COMPLETED (Serverless Service)"
----

== Commit and Trigger Pipelines Build

. From within CRW, go to the `Git` window, enter a commit message and stage the `PaymentResource` with the `+` button
+
image:checkin-message.png[]
+
. In the CRW terminal (potentially after interrupting `mvn quarkus:dev` with kbd:[ctrl + c]) run the following commands:
+
----
git commit -m"Setup payment service for serverless and EDA"
git push origin
----
+
. You will be prompted for a username and password
** *username*: `gitea`
** *password*: `gitea`
. _Immediately_ switch to the Pipelines Browser window and show that a `PipelineRun` has been triggered
+
image:pipeline-run.png[]
+
. Click on the `PipelineRun` link (highlighted above) and show the PipelineRun details
+
image:pipeline-run-details.png[]
+
. Explain the different build stages
. Click on the logs to follow
.. BETTER: in the `shell` run the following command to follow the build (and maximize shell):
+
----
tkn pr logs -L -f -n $cicd_prj
----
+
. When the pipeline finishes, it will output the URL of the knative service which we'll use in the next section
+
image:tkn-kn-url.png[]
+
. Copy the highlighted URL and set the following variable in the `shell`:
+
----
KN_URL=<URL copied from above>
----

== OPTIONAL: Single Knative Function Invoke

. Open the `Topology View` of the browser tab and ensure the knative service can be seen even when the `shell` is open
. Point out the the knative service is set to 0 instances
+
image:pre-invoke-shell.png[]
+
. Put a couple newlines in the `Kafka Payments Shell`
. In the `shell` run the following command to invoke the service:
+
----
curl -i -H 'Content-Type: application/json' -X POST -d @$DEMO_HOME/example/order-payload.json $KN_URL
----
+
. You should see the pod start up
. Once running, you should see a new entry in the `Kafka Payments Shell`
. Continue to watch the service until the pod stops and the number of instances goes back to 0

== Install Knative Eventing

. Duplicate the `Topology View` tab and switch to the `Administrator Perspective`
. Click on Operators > Installed Operators on the left and switch to the `knative-eventing` project
. Click the Knative Eventing link
+
image:knative-eventing-start.png[]
+
. Click 'Create Knative Eventing`
.. Show different options in `Form View` and explain what you can
. We can just use all the defaults, so just click `Create`
. Before moving on, make sure eventing has finished by typing the following in the shell:
+
----
oc wait --for=condition=InstallSucceeded knativeeventing/knative-eventing -n knative-eventing --timeout=6m
----
+
. It's safe to continue when the shell returns and prints:
+
----
knativeeventing.operator.knative.dev/knative-eventing condition met
----
+
. Next create a `Knative Kafka` integration by clicking on the `Knative Kafka` tab
+
image:knative-kafka-tab.png[]
+
. Click `Create KnativeKafka`
. From the `Form View`, explain that this is what allows kafka messages to be translated into knative events for the EDA we've talked about
. The eventing can be configured from the Form View.  Fill in the fields like this and click create:
** *Channel > Bootstrap Servers*: `my-cluster-kafka-bootstrap.kn-demo-dev:9092`
** *Channel > Enabled*: `true`
** *Source > Enabled*: `true`
+
image:knative-kafka-form.png[]
+
. The KnativeKafka should be created almost instantly.  You know it's safe to use once this condition can be seen
+
image:knative-kafka-install-success.png[]

== Bind Kafka Events to Knative Service

. Go back to the `Topology View` tab of the `kn-demo-dev` project
. Drag out the connector arrow from the payment knative service to add an `Event Source` as shown below:
+
image:create-event-source.png[]
+
. Fill out the Event Source Details as follows:
+
image:kafka-event-sources-1.png[]
image:kafka-event-sources-2.png[]
image:kafka-event-sources-3.png[]
image:kafka-event-sources-4.png[]
+
. Click `Create`.  You will be returned to the `Topology View`
. Open the Coolstore website is a separate (small) window.
** Make sure topology view can be seen
** Make sure Kafka Queues can be seen
+
image:suggested-layout-coolstore-event.png[]
+
. Order something from the website and click `Checkout`
. The payment knative service should spin up and the order and payment topics should have messages registered
. Switch to the Order tab in coolstore to show that this order has been processed

== Scaling and Concurrency

. From the Topology view, go to the `payment` service, open the details, and select `Edit payment` menu item
+
image:edit-payment-ksvc.png[]
+
. To remind the audience of the scaling limits, scroll down to the bottom of the ksvc details until you get to the `Advanced` section.  There click on the `Scaling` link
+
image:ksvc-scaling.png[]
+
. Show the scaling details and highlight concurrency limits
+
image:import-from-git-4.png[]
+
. Hit `Cancel` to go back to the Topology View with the payment service in focus
. Under `Display Options` select `Pod Count` to that count can be stressed
+
image:pod-count-options.png[]
+
. _Whilst keeping pod-count payment service in focus (with details), and the `Kafka Payment Shell` visible_: from the `shell` run the following command to send 50 concurrent requests to the payment service
** NOTE: `KN_URL` should already have been set from previous sections but if you don't have it you can get it with `KN_URL=$(oc get rt payment -o jsonpath='{.status.url}')`
+
----
hey -n 50 -c 50 -t 60 -m POST -D $DEMO_HOME/example/order-payload.json -T "application/json" $KN_URL
----
+
. The count on the payment ksvc should increase to near 50
. Show the `hey` histogram to get a sense of timings
+
image:scaling-and-histogram.png[]

== Revisions and Traffic Splitting

. Next, in the payment details highlight the revision list for `payment`
+
image:revision-list.png[]
+
. _Whilst keeping the revision list visible_: Add a native revision with `kn command` from the `shell`
+
----
kn service update payment -n $dev_prj --image quay.io/mhildenb/homemade-serverless-native:initial-service-1.1 --revision-name "{{.Service}}-native"
----
+
. Notice that payment-native is now set to get 100% of the traffic
** NOTE: You may need to recent payment in the `Topology View` window
+
image:new-native-revision.png[]
+
. Show that traffic is going 100% to the new native service by running the same `hey` command in the `shell`
+
----
hey -n 50 -c 50 -t 60 -m POST -D $DEMO_HOME/example/order-payload.json -T "application/json" $KN_URL
----
+
. Be sure to show the new "native" messages coming through in the `Kafka Payments Shell`
. Also show the histogram.  Should be responding a little bit faster
. Finally, let's split traffic between the two revisions, click on `Set Traffic Distribution`
. Adjust the Distribution between the previous (non-native) revision as shown:
** NOTE: Click 'Add Revision` link to add a new line
+
image:edit-revisions.png[]
+
. Click Save and go back to the `Topology View` refocusing as necessary
+
image:traffic-split.png[]
+
. _Whilst keeping pod-count payment service in focus (with details), and the `Kafka Payment Shell` visible_: from the `shell` run the same `hey` command to send 50 concurrent requests to the payment service
+
----
hey -n 50 -c 50 -t 60 -m POST -D $DEMO_HOME/example/order-payload.json -T "application/json" $KN_URL
----
+
. Show the hey histogram and `Kafka Payment Shell` messages as evidence of the split
+
image:split-histogram.png[left,300]
image:split-messages.png[right,400]

== Knative Functions

[IMPORTANT]
.Latest Kafka Certificates Needed To Run Locally
====
To make sure you have the latest kafka certs for running the Function locally, make sure you run this in all terminals that you will be running the service FIRST

----
oc get pods -n openshift-operators -o name | grep amq-streams | xargs oc delete -n openshift-operators

# Wait until Kafka is reinitialized

# Reinstate your local properties
. $DEMO_HOME/scripts/shell-setup.sh
----
====

. Switch to VSCode Desktop
+
. In the terminal, navigate to the following (empty) directory
+
----
mkdir $DEMO_HOME/coolstore/payment-func
cd $DEMO_HOME/coolstore/payment-func
----
+
. Initialize a new function by running the following command in the terminal
+
----
kn func create -l quarkus -t events
----
+
. Then override the default func.yaml with values we prepared earlier (about the destination image name, etc)
+
----
cp -f $DEMO_HOME/example/payment-func/func.yaml .
----
+
. Now remove all the highlighted files
+
image:kn-func-files-to-delete.png[]
+
. Next, open the `Function.java` file and show the stuff that was added by default
** Highly @Funq context
. Change `Input` to be of type `Order` and remove Input.java
** Explain that built into the function machinery is to map the body of a cloud event to POJOs
. Use kbd:[CMD + p] to open `payment-cloud.http` quickly
** Show the structure of the body
. Create the following POJOs or copy them into the payment-func directory (and show them) by running
+
----
cp $DEMO_HOME/example/payment-func/src/main/java/functions/Order.java $DEMO_HOME/coolstore/payment-func/src/main/java/functions
cp $DEMO_HOME/example/payment-func/src/main/java/functions/CreditCard.java $DEMO_HOME/coolstore/payment-func/src/main/java/functions
----
+
. We are needing to connect to kafka, so let's add that library to our project
+
----
mvn quarkus:add-extension -Dextensions=reactive-messaging-kafka
----
+
. Next edit the function file so that it looks like this (perhaps using link:example/payment-knative/PaymentResource.java[example/payment-knative/PaymentResource.java] as a reference on the side):
** Alternatively, run this command:
+
----
cp -f $DEMO_HOME/example/payment-func/src/main/java/functions/Function.java $DEMO_HOME/coolstore/payment-func/src/main/java/functions/Function.java
----
+
image:func-diff-1.png[]
image:func-diff-2.png[]
+
. Open the `application.properties` and notice there is only one there by default
. Add the properties necessary for connecting to kafka by overwriting it with this command
+
----
cp -f $DEMO_HOME/example/payment-func/src/main/resources/application.properties $DEMO_HOME/coolstore/payment-func/src/main/resources/application.properties
----
+
. Run the service locally to prove that it's working
. Open a new split terminal to use as the `Kafka Payment Shell` and run the following command in it
+
----
oc exec -c kafka my-cluster-kafka-0 -n $dev_prj -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----
+
. OPTIONAL: To ensure you have the latest certs for accessing the Kafka Cluster, run this command in the terminal 
. In the orginal shell, run the following to start the service running locally 
+
----
mvn quarkus:dev -Dsuspend
---- 
+
. Attach the debugger and set a breakpoint in the Function 
. Use kbd:[CMD + p] to open `payment-cloud.http` quickly and click on "Send" code lens
+
image:vscode-order-send.png[]
+
. The function should execute and the output should be seen in the `Kafka Payment Shell`
. Deploy the function using the following command:
+
----
kn func deploy -v -n $dev_prj
----
+
. When the deployment completes, switch to `Desktop 1` to show the Topology View
. Select `Application: All Applications`
. Find the function revision and kbd:[shift]-drag the revision into the focus group
. Select `Application: Focus`
. Next re-wire the kafka source
. Create the kafka event source using this command:
** [red]#NOTE: The UI for kafka event sources appears to be broken so you can't YET move the connector to the Service#
+
image:move-connector.png[left, 300]
image:move-error.png[right, 300]
+
. Instead, we'll delete it and create a new one manually in the `shell`
** NOTE: It's important you delete the old kafka source or you'll get multiple updates in the channel
----
oc delete -n $dev_prj -f $DEMO_HOME/install/knative-eventing/orders-event-source-func.yaml
oc apply -n $dev_prj -f $DEMO_HOME/install/knative-eventing/orders-event-source-func.yaml
----
+
. _Whilst making sure the `Kafka Payment Shell` and Topology View with Knative Func is visible_: Make a purchase from the coolstore.
. Notice the new message
+
image:function-final.png[]

= OLD WALKTHROUGH (2020)

== Payment Service on CRC 

In this section well show a payment service that is running to poll the kafka 

=== Setup Code Ready Containers 

. Download Code ready containers
. Unzip and move the binary to a location in your path
. Save the pull secret to a location in your home directory
. Configure the crc as follows:
+
----
crc config set cpus 8
crc config set memory 10486
crc config set pull-secret-file <PATH_TO_PULL_SECRET>
----
+
. Run `crc setup`
. Run `crc start` and once started record the login information presented

=== Optional: Show UI Installation of necessary operators

For this section you will need two windows: a command line shell and the web browser (for the console)

. From the console, run `crc console` which should show the OpenShift console in the running CRC instance
** NOTE:  Make sure you have the kube admin login password handy
. Go to _Operators > Operator Hub_
. Search for Knative and select the OpenShift Serverless Operator
. Select install (for all projects)
. OPTIONAL: Search for Kafka and select the AMQ Streams operator
. Create a new namespace called `knative-serving` and select this as the current project
. Navigate to the _Operators > Installed Operators_ tab and wait until OpenShift Serverless is successfully copied
. Click on the _Knative Serving_ link and then press the _Create Knative Serving_ button
+
image:knative-serving-cr-console.png[]
+
. A default CR YAML UI will appear. Point out some of the more interesting elements of the CR (as well as the help on the right side of the screen)
+
image:knative-cr-interesting.png[]
+
. Click the _CREATE_ button
. Wait for a bit and after a while, a new tab should appear in the left OpenShift drawer navigation, namely the _Serverless_ tab
. Click on the tab and show what's underneath
+
image:serverless-drawer.png[]

=== Build and install payment service on CRC

==== Build Payment

. Make sure the CRC is running and you have the appropriate login string
. Run the following commands in a shell
+
----
source scripts/shell-setup.sh
code .
----
+
. From a shell in VSCode, run the following to install the necessary prerequisites (NOTE the flags to the `install-prereq.sh` command.  These ensure that CRC can handle the installation)
** `homemade-serverless` is the name of the project where we'll be running this.  You can change this name if you'd like 
+
----
source scripts/shell-setup.sh
$DEMO_HOME/scripts/install-prereq.sh homemade-serverless --crc --skip-all-eventing
----
+
. Next use kbd:[CMD+p] to open `cr.yaml` file 
** Point out the timeout seconds as this will be important later
. Then apply this in the cluster
+
----
oc apply -f $DEMO_HOME/install/serverless/cr.yaml
----
+
. When that completes, then show the payment project in the explorer window and explain that this is a simple Quarkus project 
** Show the `pom.xml` file to show the libraries that go into compilation
** Explain how this is connecting to kafka outgoing for integration with payment topic
+
. Next, let's remove the aspects of the payment resource that causes it to poll the order service.  In vscode, use  to navigate to the *PaymentResource.java* file 
+
. Append a message to the COMPLETED message so that we can tell this service from others
+
image:payment-resource-string-change.png[]
+
. And delete the configuration for the incoming stream. In *application.properties* , delete (or comment out) the following lines for the Incoming stream:
+
image:payment-app-properties.png[]
+
** Explain how this is connecting to kafka outgoing for integration with payment topic
+
. Now rebuild the service locally
+
----
cd $DEMO_HOME/payment-service
export MAVEN_OPTS=" -Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled"
mvn clean package -DskipTests
----
+
. Now create a build configuration for OpenShift by running the following, but first:
** Explain that this just allows us to build and create an image that we store in the cluster using OpenShift specific functionality
** Explain that we could have just as easily built an image and pushed it up to some repository (which we'll point out later)
+
----
# Setup a binary based build for our quarkus instance
oc new-build  --image-stream="openshift/redhat-openjdk18-openshift:1.5" --binary --name=payment    
----
+
. And remotely (to upload the binary and bake it into a new image).  [blue]#NOTE: This should take about 1.5 minutes with crc cluster#
+
----
oc start-build payment --from-file target/*-runner.jar --follow 
----
+
. When the build is done, let's tag it as our initial revision
+
----
oc tag payment:latest payment:initial
----
+
. Next, show the image stream in the cluster by shifting to the [blue]#Browser# and shift-click on the _Administrator_ perspective.
+
. In that new tab, navigate to _Builds > ImageStreams_ and show that there is a new image in the image registry (reached from the _Administrator Perspective_ under _Builds > ImageStreams_):
+
image:payment-latest-image.png[]

==== Create Knative Serverless Service

. Now that we have our image tagged, let's create a knative service using that image.  
. First mention that we're using the knative CLI kn by issuing a `kn version` command
+
----
kn version
----
. We'll name the revision for the service name ({{.Service}} which will resolve to payment) and ({{.Generation}}, which should be 1)
+
[TIP]
.You can use the command line to quickly get the image stream
====
----
oc get is payment -o jsonpath="{.status.dockerImageRepository}" -n homemade-serverless
----
====
+
----
kn service create payment --image image-registry.openshift-image-registry.svc:5000/homemade-serverless/payment:initial --revision-name "{{.Service}}-{{.Generation}}"

# Which gives the output
Creating service 'payment' in namespace 'homemade-serverless':

  0.299s The Route is still working to reflect the latest desired specification.
  1.008s Configuration "payment" is waiting for a Revision to become ready.
 68.597s ...
 69.390s Ingress has not yet been reconciled.
 70.223s Ready to serve.

Service 'payment' created to latest revision 'payment-1' is available at URL:
http://payment.homemade-serverless.apps-crc.testing
----
. COPY the returned url (you'll need it in an upcoming part) especially if you've renamed the project that you're deploying to
+
** Show these aspects in the UI
+
image:knative-payment-revisions.png[]
+
** Show that the payment service is at 0 from the _Topology_ of the _Developer Perspective_
+
image:knative-developer.png[]
+
. Demonstrate that the service handling http requests invoking the service via curl
.. Open a [blue]#new terminal window (Watch Window)# that can be used to watch the payments topic and run this command
+
----
oc exec -c kafka my-cluster-kafka-0 -n homemade-serverless -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----
+
.. Make sure [blue]#Watch Window# is open and watching the payments topic
.. Make sure the [blue]#Browser# window with the _Developer Perspective_ can still be clearly seen
+
.. From the [blue]#Main shell# execute the following `curl` command
+
----
curl -i -H 'Content-Type: application/json' -X POST --data-binary @$DEMO_HOME/example/order-payload.json  http://payment.homemade-serverless.apps-crc.testing/
----
+
.. Show payment container spinning up
+
image:payment-spin-up.png[]
+
.. Show payment info being pushed to the queue
+
.. Then show it scaling back down to 0

== Demonstrate Knative Eventing (Remote Cluster)

[WARNING]
====
The coolstore and Knative Eventing require more horsepower than CRC can currently provide.  For this part of the demo you will need a separate external cluster running.

You can setup the coolstore by running the following commands after logging into the cluster
----
. scripts/shell-setup.sh
$DEMO_HOME/scripts/install-coolstore.sh -p coolstore
----

Wait until all the components have been installed.

_NOTE: You can cause cluster side rebuilds of all the components (instead of updating images to point to dockerhub) by using the `--rebuild` flag_
====

[red]#When you login to this cluster, be sure to record the context as remote by using this command#
----
oc config rename-context $(oc config current-context) remote-context
----

=== Screen and Window Setup

==== Screen 1

Here are how the windows should be laid out on Screen 1

. [blue]#Topology View#: A browser window with the _TopologyView_ of the _Developer Perspective_ open
. [blue]#Watch Window#: A new terminal windowthat can be used to watch the payments topic and runs this command:
+
----
oc exec -c kafka my-cluster-kafka-0 -n coolstore -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----
+
. [blue]#Coolstore#: A browser window that has the coolstore open so that an order can be placed
. Your windows should look something like this:
+
image:recommended-layout.png[]

==== Screen 2

On the other screen you should have a full screen view of VSCode, which you started from the `code .` command run from the _$DEMO_HOME_ directory.

=== Demonstrate the working Coolstore (without payment)

. First explain that we are now running on a separate cluster and that our coolstore with all its microservices are setup.
. Login to the OpenShift console, switch to the `coolstore` project and select the Developer Perspective and show all the different services 
+
image:.png[]
+
. Demonstrate the coolstore site working by kbd:[SHIFT] clicking the launcher icon on the Developer Perspective (see previous) and moving the newly opened window to the right 
** NOTE: you can also find the URL like this:
+
----
oc get route coolstore-ui -n coolstore
----
+
** [blue]#NOTE: You may find that it takes the page a while to load the first time, and also that the inventory might not show.  If this happens, just press reload#
+
image:add-to-cart.png[]
+
. From the cart, checkout and then enter credit card details (any 16-digit number beginning with 4 will work)
+
image:checkout.png[]
+
. Now navigate to the orders page.  Notice that the order gets filed but that it *doesn't* get processed
+
image:orders.png[]

=== Setup Knative Eventing

Now we want to use events the order topic to be our *source* (see also link:https://knative.dev/docs/eventing/samples/kafka/source/index.html[here] for generic example) and use the payment service as our *sink*

[WARNING]
====
The setup script should have handled this when setting up the coolstore project, but the Knative Eventing and Knative Kafka Eventing Operators should be installed on the cluster.  A good way to check this is to run this command in the `coolstore` project

----
oc get pods | grep -i ^kafka
----

You should see the following:
----
kafka-ch-controller-57cf94b477-dk9ss          1/1     Running     0          73s
kafka-controller-manager-56d58bb444-dtpkd     1/1     Running     0          81s
kafka-webhook-77b75f7c7f-df7vb                1/1     Running     0          72s
----
====

. Show all the installed operators
+
image:operators-all-necessary-installed.png[]
+
. Show the setup for the `Knative Eventing Kafka` by clicking on the highlighted link in the previous image, then clicking on the knative-eventing-kafka instance 
** Point the "bootstrapServers" in the resulting _Overview_
+
image:knative-eventing-kafka.png[]
+
. Now we create a simple event binding to the kafka event *source* to the payment service *sink*.  Use kbd:[CMD + p] to quickly open the `kafka-event-source.yaml`
+
image:kafka-event-source.png[]
+
. Apply that source to the cluster
+
----
oc apply -f $DEMO_HOME/payment-service/knative/kafka-event-source.yaml 
----
+
. Check to see if the event source is running.  It won't be running yet since the payment *sink* does not exist yet on this cluster
+
----
oc get pods -l eventing.knative.dev/SourceName=kafka-source-orders
----
+
. You can also refresh the orders page on the coolstore site and show that the payment is still not processed

=== Create a Payment Image on Coolstore Cluster

We need to find a way to get the image to our coolstore cluster.  Choose one of the following options to get it there:

. <<OPTION 1: Add the serverless payment service via skopeo,Copy from Destination Cluster>>
. <<OPTION 2: Add the serverless payment service via S2I,Build (native) image on cluster from S2I>>

===== OPTION 1: Add the serverless payment service via skopeo

[NOTE]
====
You will need to use kubernetes contexts to get this to work.  Use this command to list all the current contexts

----
oc config get-contexts
----

This will return a bunch of contexts that are defined.  You will want to find the NAME that is associated with your CRC cluster and store the whole of the name in `SRC_CLUSTER_CTX`.  Then find the remote cluster and store its NAME in `REMOTE_CLUSTER_CTX`.  If you marked your contexts as you logged into the different clusters this might look like this:

----
REMOTE_CLUSTER_CTX="remote-context"
SRC_CLUSTER_CTX="crc-context"
----
====

. Get the user and token from the coolstore (remote) cluster.  Assuming you are logged into the cluster with a token on the command line then issue the following commands
** [red]#NOTE: you can't use the password here.  It's a bearer token type login for the registry# 
** [red]#NOTE: the default login for the crc cluster is kube:admin, but the extra `:` confuses skopeo.  Thus we need to make sure to take out that `:` with `sed` before setting it as the src username#
+
----
oc config use-context $SRC_CLUSTER_CTX
SRC_CREDS="$(oc whoami | sed s/\://g):$(oc whoami -t)"
SRC_REPO="$(oc get route default-route -n openshift-image-registry -o jsonpath='{.spec.host}')"
oc config use-context $REMOTE_CLUSTER_CTX
REMOTE_CREDS="$(oc whoami):$(oc whoami -t)"
REMOTE_REPO="$(oc get route default-route -n openshift-image-registry -o jsonpath='{.spec.host}')"
----
+
. Next issue the `skopeo` command to copy the image from the src clusters image stream to the destination.
** NOTE: If there is difficulty, you can add a `--debug` just before the `copy` subcommand to see what's going on.  For instance, you may need to add a `src-` or `dest-` `tls-verify=false`
+
----
skopeo copy --src-creds ${SRC_CREDS} --src-tls-verify=false --dest-creds ${REMOTE_CREDS} docker://${SRC_REPO}/homemade-serverless/payment:initial docker://${REMOTE_REPO}/coolstore/payment:initial
----
+
[TIP]
====
If the image already exists on the cluster and you want to show again copying to the cluster, then you can do the following to remove the docker image layers:

. Remove any references to the image by removing image streams that point to it:
+
----
oc delete is payment
----
+
. Then, when you're sure there is nothing referencing the image in question, run this command (assuming `REMOTE_REPO` is still set from above)
+
----
oc adm prune images --registry-url=https://${REMOTE_REPO} --confirm
----
====
+
. Once the command completes, you should be able to navigate to the _Image Stream_ tab of the `coolstore` project in the destination cluster and see the image there
+
image:imagestream-payment-dest.png[]

==== OPTION 2: Add the serverless payment service via S2I

Let's create a quarkus native service to handle payment and use the power of the cluster to compile this

. Create a new Source to Image (S2I) build
+
----
oc new-build quay.io/quarkus/ubi-quarkus-native-s2i:19.2.0~https://github.com/hatmarch/serverless-demo.git --context-dir=payment-service --name=payment-native \
    -e MAVEN_OPTS=" -Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled"
oc cancel-build bc/payment-native
oc patch bc/payment-native -p '{"spec":{"resources":{"limits":{"cpu":"4", "memory":"6Gi"}}}}'
oc start-build bc/payment-native --follow
----
+
** Discuss that we're updating the build command to have more omph for building the native service
** whilst the command is running explain the different aspects of the command such as the builder image and the git repo reference
+
. Once the build has completed, tag the resulting image
+
----
oc tag payment-native:latest payment:initial-native
----
+
. You should now be able to see the image in the _ImageStream_ for payment in the `coolstore`

=== Create Payment Knative Service (on Remote)

Once you have a `payment:initial` image in the `coolstore` we need to add the service

. Next create a new knative payment knative service (as we did previously on the CRC instance)
** NOTE: the -l flag is a label that will allow the service to show up as part of the "focus" topology
** TIP: if you want to set the concurrency limit per revision, you can use the `--concurrency-limit=2` flag
+
----
kn service create payment --image image-registry.openshift-image-registry.svc:5000/coolstore/payment:initial --revision-name "{{.Service}}-{{.Generation}}" -l app.kubernetes.io/part-of=focus 
----
+
. Demonstrate the the service is ready (and dormant) by showing the topology view
+
image:topology-coolstore-payment.png[]
+
. Show that the `kafka-source-orders` has also spun up.  You can do this either by viewing the _Topology View_ of the _Developer Perspective_ or by issuing the following command:
+
----
oc get pods -l eventing.knative.dev/SourceName=kafka-source-orders
----
+
. You might also show that the order we had in our queue has now been processed

=== Demonstrate event driven serverless

. First make sure your screens are arranged as suggested in <<Screen and Window Setup,Screen and Window Setup>>
. Next, use the coolstore site to order something
+
image:checkout.png[]
+
. Upon checkout you should see the payment pod spinning up to consume the order in the [blue]#Watch Window#
+
image:consuming-kafka-queue.png[]
+
. You can then go to the *Orders* section of the site to show that the order was consumed.  When you return to the [blue]#Topology View# the pod should be spun down (with a clear or black outline).
+
. Attempt to make a second order before the service spins down, notice that it's processed immediate
+
. Finally watch the topology view until the service spins down to nothing. 

=== Demonstrating Concurrency

. Show the concurrency limit on the service by selecting the _KSVC_ in the Topology view and selecting the revision
** ALTERNATIVELY: if you have not set this on a per revision basis, you can show the global setting the knative instance
+
image:knative-revision-concurrency.png[]
+
. Open a new tab with the _Topology View_ of the _Developer Perspective_
. Run the following `hey` command to show the payment service running under load
** NOTE: information on the `hey` command can be found link:https://github.com/rakyll/hey[here]
+
----
hey -n 100 -c 100 -m POST -D $DEMO_HOME/example/order-payload.json -T "application/json" $(oc get rt payment -o jsonpath='{.status.url}')
----
+
. Navigate to the Topology view while the command is running:
** Notice number of containers that are spun up, this should be <NUM_REQUESTS>/<MAX_CONCURRENCY>
. When the `hey` command report comes back:
** Notice the timings
+
image:hey-example-timings.png[Example timings]


=== Demonstrating Knative Revisions (featuring native Quarkus)

[WARNING]
====
If you are running linux in a container, you need to make sure the docker daemon has enough memory assigned to it, otherwise the native quarkus build will fail towards the end of the run.  This configuration seemed to be enough to build the payment-service:

image:docker-requirement.png[]
====

. Use kbd:[CMD+p] to quickly open the `PaymentResource.java` and update the _COMPLETED_ message in the `pass` function:
+
image:payment-completed-log.png[]
+
. Next, build a native image (locally).
** If you would like to build the image using S2I, you'll need to first checkin the changes and see <<,these instructions>>
+
----
cd payment-service
export MAVEN_OPTS=" -Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled"
mvn clean package -Pnative -DskipTests
----
+
. Next, add a build to our project that will allow us to create an image out of the binary we just compiled.
+
----
oc new-build quay.io/quarkus/ubi-quarkus-native-binary-s2i:19.2.0 --binary --name=payment-native
----
+
Next, start the (binary) build of the image using our new configuration
+
----
oc start-build payment-native --from-file $DEMO_HOME/payment-service/target/*-runner --follow
----
+
. When finished, then tag this latest build as a `payment:quarkus-native` build
+
----
oc tag payment-native:latest payment:quarkus-native
----
+
. Next, update our payment knative service to use the quarkus-native image we just created (keeping concurrency limits the same)
** NOTE: if you don't want to write out the location to the image registry, you can use this embedded oc command after the `--image` switch
+
----
oc get istag/payment:quarkus-native -o jsonpath='{.image.dockerImageReference}'
----
+
----
kn service update payment --image $(oc get is/payment -o jsonpath='{.status.dockerImageRepository}'):quarkus-native --revision-name "{{.Service}}-{{.Generation}}"
----
. Show revisions in developer console
+
image:knative-revisions.png[]
+
. Run the following `hey` command to show the payment service running under load
** NOTE: information on the `hey` command can be found link:https://github.com/rakyll/hey[here]
+
----
hey -n 100 -c 100 -m POST -D $DEMO_HOME/example/order-payload.json -T "application/json" $(oc get rt payment -o jsonpath='{.status.url}')
----
+
. Navigate to the Topology view while the command is running:
** Notice number of containers that are spun up, this should be <NUM_REQUESTS>/<MAX_CONCURRENCY>
. When the `hey` command report comes back:
** Notice the timings
+
image:hey-example-timings-quarkus.png[Example timings with quarkus native]
+
. Update the traffic in the _Topology View_ back to the initial revision as per the instructions in the screenshot
+
image:knative-update-traffic-distrubtion.png[]
+
. Run `hey` again
. Switch back to the _Topology View_ and notice that revision 1 is getting the traffic
+
image:knative-back-to-initial-revision.png[]
+
. Look back at the `hey` results
** Notice that the timings are now back in line with the initial revision


== Troubleshooting ==

=== 500 errors

You may notice 500 errors, particularly if you send multiple requests under load:

image:500-errors.png[]

I believe this is because there is currently a race condition when the second request hits a pod where the payment topic (`producer` in the code) is not fully setup in the payment service (thus a null pointer).  Looks like the first exception happens in the `pass` function but this is caught in the handleCloudEvent function, only for the `fail` event to use the `producer` null pointer to try to log a failure at which time a new uncaught exception is raised.

If you set the concurrently limit to 1, you should be able to demonstration that this error doesn't happen with hey

=== Getting logs of Knative service

The epheral nature of the knative service can make it hard to capture logs of the service, particularly if you notice that the service had issues after it's gone.

Aside from setting up Elasticsearch to retain all logs, you can consider using `stern` in the background.  Using the .devcontainer that is run from within VSCode, you can have the following command running in a background terminal:

----
stern -l serving.knative.dev/service=payment
----

To see all the logs from revision 1 of the payment service (-1 represents the revision number I believe).  This command will include logs from all containers associated with the pod (such as `queue-proxy`).  If you only want the deployed code itself to log, add the `-c user-container` flag

=== Viewing and Modifying Order (MongoDB) Database

You cannot connect to the mongodb instance using the latest plain adminer container.  Instead you need to follow the special instructions below.  If you my version of adminer does not work for you, you can attempt to follow <<Updating your own Adminer image,these instructions>> for creating a new image yourself from the latest.

. Start port forwarding to the mongodb service
+
----
oc port-forward -n coolstore svc/order-database 27017:27017
----
+
. Run the modified adminer pod
** NOTE: `quay.io/mhildenb/myadminer:1.1` is a version 4.7.6 adminer container that I've updated to support this
+
----
docker run -p 8080:8080 -e ADMINER_DEFAULT_SERVER=docker.for.mac.localhost quay.io/mhildenb/myadminer:1.1
----
+
. Login as shown
+
image:adminer-mongo-password.png[]
+
. You should now have access to the mongo database with the ability to list and edit entries:
+
image:adminer-mongo-edit.png[]

==== Updating your own Adminer image

There are two reasons why the normal adminer image cannot connect to the mongo database:

1. It requires a newer version of php integration with MongoDB
2. The mongoDB is not setup with a user and a password (Adminer does not allow access to such databases by default for security reasons)

To update the latest adminer image to be able to connect to the userless MongoDB follow these instructions:

. Run an instance of the adminer container as follows:
+
----
docker run -it -u root --name my_adminer adminer:latest sh 
----
** NOTE: If an instance of the container is already running you can use the `docker exec -it` command instead
+
. Then from inside the container run
+
----
apk add autoconf gcc g++ make libffi-dev openssl-dev
pecl install mongodb
echo "extension=mongodb.so" > /usr/local/etc/php/conf.d/docker-php-ext-mongodb.ini
----
+
. Next add a plugin as per link:https://nerdpress.org/2019/10/23/adminer-for-sqlite-in-docker/[This site].  It will require you to create a login-password-less.php file in the `/var/www/html/plugins-enabled/` directory
+
[CONTENTS]
====
----
<?php
require_once('plugins/login-password-less.php');

/** Set allowed password
 * @param string result of password_hash
 */
return new AdminerLoginPasswordLess(
    $password_hash = password_hash("admin", PASSWORD_DEFAULT)
);
----
====
+
. now commit this container as a new image
+
----
docker commit my_adminer myadminer:1.1    
----

=== Insecure ImageRegistry ===

Might be solved as per link:https://github.com/knative/serving/issues/2136[here] but can't get the controller pod to take the new environment variable

Looks like it has something to do with the labels.  If the sha is used instead it seems to work properly.  You can find the sha like this:
----
$ oc get istag/payment:latest -o jsonpath='{.image.dockerImageReference}'
image-registry.openshift-image-registry.svc:5000/user1-cloudnativeapps/payment@sha256:21ca1acc3f292b6e94fab82fe7a9cf7ff743e4a8c9459f711ffad125379cf3c7
----

And then apply it as a service like this:
----
kn service create payment --image $(oc get istag/payment:initial-build -o jsonpath='{.image.dockerImageReference}') --label "app.kubernetes.io/part-of=focus" --revision-name "{{.Service}}-{{.Generation}}" --annotation sidecar.istio.io/inject=false --force
----

----
oc port-forward <image-registry-pod> -n openshift-image-registry 5001:5000
----

To get the cert as a pem file, do this:
----
openssl s_client -showcerts -connect localhost:5001 </dev/null 2>/dev/null|openssl x509 -outform PEM >mycertfile.pem
----

== Appendix

=== Copying OpenShift images to public repositories

If you have images that you've compiled on an OpenShift cluster and you want to pull them out of the local image stream to something like `quay.io`, you can use one of the following approaches to copy the images out of openshift.  Both use the `skopeo` command which is installed by default in the .devcontainer.  

For both examples, it assumes the copying of a payment service.  As such, note the following for the different variables:

* USER: your username for the public repository
* PASSWORD: your password or TOKEN for the public repository
* PROJECT: the project your image stream lives in (such as coolstore)
* IMAGE_DEST: Replace this with your repository, project, image-name, and version, example: `quay.io/mhildenb/homemade-serverless-java:1.0`: 

==== Image Registry is exposed publicly 

You need only run the following command:

----
skopeo copy --src-creds "$(oc whoami):$(oc whoami -t)" --dest-creds "${USER}:${PASSWORD}" docker://$(oc get is payment -o jsonpath='{.status.publicDockerImageRepository}'):latest docker://{IMAGE_DEST}       
----

==== Image Registry is private

If instead you need to copy from an image registry that is not exposed outside the cluster, you must instead do the following:

. Port forward to openshift's internal image registry
+
----
oc port-forward svc/image-registry -n openshift-image-registry 5000:5000
----
+
. Then in a separate shell, run the following command
+
----
skopeo copy --src-creds "$(oc whoami):$(oc whoami -t)" --src-tls-verify=false --dest-creds "${USER}:${PASSWORD}" docker://localhost:5000/${PROJECT}/payment:latest docker://{IMAGE_DEST}
----

=== Install the initial payment MIRCO service 

This section is necessary if you're wanting to show the conv

. Run the following commands in a shell
+
----
. scripts/shell-setup.sh
code .
----
+
. From a shell in VSCode, run the following to install the necessary prerequisites (NOTE the `--crc` flag)

+
----
. scripts/shell-setup.sh
$DEMO_HOME/scripts/install-prereq.sh homemade-serverless --crc 
----
+
. When that completes, then install the payment service
+
----
$DEMO_HOME/scripts/install-payment.sh
----
+
. Next use the crc _Developer Perspective_ and _Topology_ to show what is currently in our project
+
image:developer-payment-alone.png[]
+
.. Explain that the payment service will watch the orders topic and "process that" and put the output on the payments topic
.. Show that there is one instance of the payment service running all the time
.. Show the different kafka nodes
+
. Next demonstrate how the payment service currently interacts with the kafka queues by setting up two windows
+
.. *Terminal Window 2* Run the following command to watch the payments:
+
----
oc exec -c kafka my-cluster-kafka-0 -n user1-cloudnativeapps -- /opt/kafka/bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic payments
----
+
.. *Terminal Window 1* Run the following command to simulate an order being placed by the coolstore
+
----
cat $DEMO_HOME/example/order-payload.json | oc exec -i -c kafka my-cluster-kafka-0 -n user1-cloudnativeapps -- /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic orders
----
+
. You should now see the order being processed as seen here:
+
image:payment-kafka-test.png[]

=== Convert Payment Service to Serverless 

Now lets wrap our payment service in a knative service.  This will allow knative to manage the container and decide when new containers should be started.  In wrapping it in a service, we're expecting it to no longer need to poll the kafka topic.

. First stop our payment service from being run all the time (by deleting a deployment) and remove all connections to it
+
----
oc delete dc/payment route/payment svc/payment
----
+
. Next, let's remove the aspects of the payment resource that causes it to poll the order service.  In vscode, use kbd:[CMD+p] to navigate to the *PaymentResource.java* file 
+
. Delete (or comment out) the onMessage() method:
+
image:onMessage.png[]
+
. And delete the configuration for the incoming stream. In *application.properties* , delete (or comment out) the following lines for the Incoming stream:
+
image:payment-app-properties.png[]
+
** Explain that this is no longer necessary because instead the event will trigger the starting of a container with the event as the incoming context.
+
. Now rebuild the service locally
+
----
cd $DEMO_HOME/payment-service
export MAVEN_OPTS=" -Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M \
  -XX:+CMSClassUnloadingEnabled"
mvn clean package -DskipTests
----
+
. And remotely (to upload the binary and bake it into a new image).  [blue]#NOTE: This should take about 1.5 minutes with crc cluster#
+
----
oc start-build payment --from-file target/*-runner.jar --follow 
----
+
** When the build is done, notice that there is a new image in the image registry (reached from the _Administrator Perspective_ under _Builds > ImageStreams_):
+
image:payment-latest-image.png[]
+
. Now we want to specially tag this image as not using kafka
+
----
oc tag payment:latest payment:nokafka
----
+
. Next let's create a knative service using the image we just tagged.  We'll name the revision for the service name ({{.Service}} which will resolve to payment) and ({{.Generation}}, which should be 1)
+
[TIP]
.You can use the command line to quickly get the image stream
====
----
oc get is payment -o jsonpath="{.status.dockerImageRepository}" -n user1-cloudnativeapps
----
====
+
----
kn service create payment --image image-registry.openshift-image-registry.svc:5000/user1-cloudnativeapps/payment:nokafka --revision-name "{{.Service}}-{{.Generation}}"

# Which gives the output
Creating service 'payment' in namespace 'user1-cloudnativeapps':

  0.299s The Route is still working to reflect the latest desired specification.
  1.008s Configuration "payment" is waiting for a Revision to become ready.
 68.597s ...
 69.390s Ingress has not yet been reconciled.
 70.223s Ready to serve.

Service 'payment' created to latest revision 'payment-1' is available at URL:
http://payment.user1-cloudnativeapps.apps-crc.testing
----
+
** Show these aspects in the UI
+
image:knative-payment-revisions.png[]
+
** Show that the payment service is at 0 from the _Topology_ of the _Developer Perspective_
+
image:knative-developer.png[]
+
. Demonstrate that the service handling http requests by invoking using the RESTClient extension in vscode.  
.. Make sure *Terminal Window 2* (from above) is still open and watching the payments topic.
.. Use kbd:[CMD+p] and enter `payment.http` to open this file quickly
.. Click the "code-lens" above the first instance to post to the service
+
image:payment-restclient.png[]
+
.. Switch quickly to the _Developer Perspective_ to show the service spinning up
+
image:payment-spin-up.png[]
+
.. Switch back to the shell showing the kafka queue
+
.. Then show it scaling back down to 0


=== Update Payment Service 

. Edit the PaymentResource so that the success output shows some sort of change

. Compile locally with the following command
+
----
cd $DEMO_HOME/payment-service
mvn package -DskipTests
----
+
. When the compilation is done, upload the binary to make a new image ([blue]#NOTE: This should take about 1.5 minutes with crc cluster)#
+
----
oc start-build payment --from-file target/*-runner.jar --follow
----
+
. Then update the service with the newest revision
+
----
kn service update payment --image $(oc get istag/payment:latest -o jsonpath='{.image.dockerImageReference}') --revision-name "{{.Service}}-{{.Generation}}"
----
+
. Demonstrate that there is a new revision that is taking traffic
+
----
$ kn service describe payment
Name:       payment
Namespace:  user1-cloudnativeapps
Age:        4h
URL:        http://payment.user1-cloudnativeapps.apps-crc.testing

Revisions:  
  100%  @latest (payment-5) [5] (29s)
        Image:  image-registry.openshift-image-registry.svc:5000/user1-cloudnativeapps/payment:latest (pinned to 64a5a8)

Conditions:  
  OK TYPE                   AGE REASON
  ++ Ready                  20s 
  ++ ConfigurationsReady    21s 
  ++ RoutesReady            20s 
----
+
** the _pinned to_ field should match the first characters of the sha for the image labelled `:latest`
+
image:find-image.png[]


=== Deleting Knative Services

Though knative services are reported from oc get svc and oc get rt, you cannot delete them in this way.  Instead you must delete them based on the distinct (knative) descriptor that they have

Here is a command to delete all services (exposed as routes) that have the name that includes 'payment'
----
oc delete services.serving.knative.dev $(oc get rt --no-headers | grep -i payment | awk '{print $1}')
----

=== Alternative Knative service creation (without `kn` client)

Knative Services can be created not just using the command line but also by creating resources using yaml

----
oc apply -f payment-service/knative/knative-serving-service.yaml 
----

It will take a while for ingress to be configured for the service.  You can issue this command and you should see the following output:

----
$ watch oc get rt
NAME      URL                                                                           READY   REASON
payment   http://payment.user1-cloudnativeapps.apps.service-mesh-demo.openshifttc.com   True    
----

=== Trigger Knative Service with Kafka Event

Now we want to use events the order topic to be our *source* (see also link:https://knative.dev/docs/eventing/samples/kafka/source/index.html[here] for generic example) and use the payment service as our *sink*

[NOTE]
====
First we need to install the Kafka knative event source operator as seen below.  Go to link:demo-setup.adoc[here] for more details

image:kafka-event-operator.png[]
====

. Create an instance of kakfa eventing for our namespace.  To do this call
+
----
$ oc apply -f $DEMO_HOME/install/kafka-eventing/kafka-eventing.yaml 
knativeeventingkafka.eventing.knative.dev/knative-eventing-kafka created

$ oc wait --for=condition=InstallSucceeded KnativeEventingKafka knative-eventing-kafka
----
+
.. When the command completes, the following pods will be able to be seen (with the following command):
+
----
$ watch "oc get pods | grep -i ^kafka"

kafka-ch-controller-57cf94b477-dk9ss          1/1     Running     0          73s
kafka-controller-manager-56d58bb444-dtpkd     1/1     Running     0          81s
kafka-webhook-77b75f7c7f-df7vb                1/1     Running     0          72s
----
+
. Then use the yaml file to bind the kafka event source to the payment service sink (show what's going on first)
+
image:kafka-event-source.png[]
+
----
oc apply -f $DEMO_HOME/payment-service/knative/kafka-event-source.yaml 
----
+
. Check to make source the event source is running
+
----
$ oc get pods -l knative-eventing-source-name=kafka-source
----
+
. Demonstrate event driven serverless
+
.. Make sure the _Developer Perspective_ can be seen
.. Have *Terminal Window 2* up showing what gets written to the payment queue
.. In *Terminal Window 1* run the following command to simulate an order coming in from the order service
+
----
cat $DEMO_HOME/example/order-payload.json | oc exec -i -c kafka my-cluster-kafka-0 -n user1-cloudnativeapps -- /opt/kafka/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic orders
----
+
.. Show that the service gets spun up and show the results of the processing in the payment queue

== Knative payment on full cluster (integration with website) 

=== Install the coolstore project 

WARNING: You will need to log into a real OpenShift cluster to run the coolstore website.  The requirements are too heavy to run it with crc (as of verion 1.8)

. Locally log into the cluster with `oc login` command
. Setup local environment
+
----
source scripts/shell-setup.sh
----
+
. Run the coolstore setup script. NOTE: You can cause cluster side rebuilds of all the components (instead of updating images to point to dockerhub) by using the `--rebuild` flag
+
----
$DEMO_HOME/scripts/install-coolstore.sh -p coolstore
----
+
. It will take a little while for all the pods to be deployed (and images downloaded)
. Test the deployment by getting the route
+
----
echo "http://$(oc get route coolstore-ui -o jsonpath='{.spec.host}')/"
----

=== Building the Payment Service

This command does not build the payment service as it's assumed that the it will get changed.  But if you need to create the payment service

----
cd $DEMO_HOME/payment-service
export MAVEN_OPTS="-Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled"
mvn clean package -DskipTests
----

When the build is done, deploy with:

----
oc start-build payment --from-file target/*-runner.jar --follow
----

Finally, be sure to tag the payment build (we'll need this for service revisions later)
----
oc tag payment:latest payment:original
----

=== Remove direct Knative integration code from original payment service

Currently our Payment service directly binds to Kafka to listen for events. Now that we have Knative eventing integration, we no longer need this code. CMD+p to navigate to the *PaymentResource.java* file 

Delete (or comment out) the onMessage() method:

image:onMessage.png[]

And delete the configuration for the incoming stream. In *application.properties* , delete (or comment out) the following lines for the Incoming stream:

image:payment-app-properties.png[]

Explain that this is no longer necessary because instead the event will trigger the starting of a container with the event as the incoming context.

Now rebuild the service locally
----
cd $DEMO_HOME/payment-service
export MAVEN_OPTS=" -Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled"
mvn clean package -DskipTests
----

And remotely

----
oc start-build payment --from-file target/*-runner.jar --follow
----

Then tag this new image

----
oc tag payment:latest payment:noqueue
----

And update our revision to use the image we just tagged.  We'll name the revision for the service name ({{.Service}} which will resolve to payment) and ({{.Generation}}, which should be 2)

----
kn service update payment --image $(oc get istag/payment:noqueue -o jsonpath='{.image.dockerImageReference}') --revision-name "{{.Service}}-{{.Generation}}"
----

Demonstrate that there is a new revision that is taking traffic

----
$ kn service describe payment
Name:         payment
Namespace:    user1-cloudnativeapps
Labels:       app.kubernetes.io/part-of=focus
Annotations:  sidecar.istio.io/inject=false
Age:          36m
URL:          http://payment.user1-cloudnativeapps.apps.service-mesh-demo.openshifttc.com
Address:      http://payment.user1-cloudnativeapps.svc.cluster.local

Revisions:  
  100%  @latest (payment-3) [3] (1m)
        Image:  image-registry.openshift-image-registry.svc:5000/user1-cloudnativeapps/payment:noqueue (pinned to 21ca1a)

Conditions:  
  OK TYPE                   AGE REASON
  ++ Ready                   1m 
  ++ ConfigurationsReady     1m 
  ++ RoutesReady             1m 
----

=== Recompile the payment service (native quarkus)

[WARNING]
.Resource Requirements for Docker Quarkus Build
====
If you are running linux in a container, you need to make sure the docker daemon has enough memory assigned to it.  This configuration seemed to be enough to build the payment-service

image:docker-requirement.png[]
====

----
cd payment-service
export MAVEN_OPTS=" -Xmx1024M -Xss128M -XX:MetaspaceSize=512M -XX:MaxMetaspaceSize=1024M -XX:+CMSClassUnloadingEnabled"
mvn clean package -Pnative -DskipTests
----

While that's compiling, in another VSCode terminal, update our builder to be able to build native quarkus

----
oc delete bc payment
oc new-build quay.io/quarkus/ubi-quarkus-native-binary-s2i:19.2.0 --binary --name=payment -l app=payment
----

Once the native maven build is done, we can start a build using our new configuration

----
cd $DEMO_HOME/payment-service
oc start-build payment --from-file target/*-runner --follow
----

Then tag this latest build accordingly

----
oc tag payment:latest payment:quarkus-native
----

Then update our knative service (tagging revisions before and after updating the service)

----
kn service update payment --tag @latest=traditional
kn service update payment --image $(oc get istag/payment:quarkus-native -o jsonpath='{.image.dockerImageReference}') --revision-name "{{.Service}}-{{.Generation}}"
kn service update payment --tag @latest=native
----

=== Knative Revisions

Start with traffic to the original version

----
$ kn service update payment --traffic traditional=100
Updating Service 'payment' in namespace 'user1-cloudnativeapps':

  0.275s Ingress has not yet been reconciled.
  1.401s Ready to serve.

Service 'payment' updated with latest revision 'payment-4' (unchanged) and URL:
http://payment.user1-cloudnativeapps.apps.cluster-nab-92c5.nab-92c5.example.opentlc.com

export SVC_URL=$(oc get rt payment -o template='{{ .status.url }}')

$ curl $SVC_URL -s -o /dev/null -w "%{time_starttransfer}\n"
12.305205
$ curl $SVC_URL -s -o /dev/null -w "%{time_starttransfer}\n"
0.546286
----

Now look at the quarkus native version

----
$ kn service update payment --traffic native=100
export SVC_URL=$(oc get rt payment -o template='{{ .status.url }}')

$ curl $SVC_URL -s -o /dev/null -w "%{time_starttransfer}\n"f 
10.930526
$ curl $SVC_URL -s -o /dev/null -w "%{time_starttransfer}\n"
0.543870
----

TODO: See link:https://blog.openshift.com/knative-configurations-routes-and-revisions/[here]

TODO: See link:https://redhat-developer-demos.github.io/knative-tutorial/knative-tutorial-basics/0.7.x/02-basic-fundas.html#deploying-new-revision[here]

Tagging in ImageStream
----
oc tag payment@sha256:573f369a858c692b71f02acb470b321816d8ff8ababece8148ac8c939a37c9e2 payment:java
----

=== Service Autoscaling

NOTE: _The knative-serving attribute scale-to-zero-grace-period is a “dynamic parameter” i.e. any updates to this value are reflected immediately to all its consumers; while all other parameters are static parameters i.e. change to it need a restart of the autoscaler deployment of knative-serving namespace._

TODO: See link:https://knative.dev/docs/serving/samples/autoscale-go/index.html[here]
Then reopen the website

=== Service Pinning

TODO: See link:https://redhat-developer-demos.github.io/knative-tutorial/knative-tutorial-basics/0.7.x/02-basic-fundas.html#_service_pinned_to_first_revision[here]
